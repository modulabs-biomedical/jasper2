<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>http://localhost:20010/</title>
   
   <link>http://localhost:20010/</link>
   <description>모두의연구소 바이오메디컬랩 기술 블로그입니다.</description>
   <language>en-uk</language>
   
   <title>
   <![CDATA[ 바이오메디컬랩@모두의연구소 ]]>
   </title>
   <description>
   <![CDATA[ 모두의연구소 바이오메디컬랩 기술 블로그입니다. ]]>
   </description>
   <link>http://localhost:20010/</link>
   <image>
   <url>http://localhost:20010/assets/images/favicon.png</url>
   <title>바이오메디컬랩@모두의연구소</title>
   <link>http://localhost:20010/</link>
   </image>
   <generator>Jekyll 3.6.2</generator>
   <lastBuildDate></lastBuildDate>
   <atom:link href="http://localhost:20010/rss.xml" rel="self" type="application/rss+xml"/>
   <ttl>60</ttl>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>FusionNet</title>
	  <link>http://localhost:20010/FusionNet</link>
		
				
		
				
		
				
		
				
		
				
		
				
		
				
						<author>김경모</author>
				
		
				
		
				
		
	  <pubDate>2018-05-04T09:00:00+09:00</pubDate>
	  <guid>http://localhost:20010/FusionNet</guid>
	  <description><![CDATA[
	     <h1 id="fusionnet"><strong>FusionNet</strong></h1>

<p>FusionNet은 U-Net처럼 Semantic Segmentation에 활용 할 수 있는 모델입니다.</p>

<p>이름이 FusionNet인 이유는 아마도 Encoder에 있는 Layer를 가져와 Decoder에 결합(Fusion)하는 방법이 이 모델에 가장 특징적인 부분이기 때문인 것 같습니다.</p>

<p>U-Net과 유사한 부분이 많기 때문에 블로그에 U-Net글을 읽으시면 많은 도움이 될 것 같습니다. 
<a href="https://modulabs-biomedical.github.io/U_Net">U-Net (by 강은숙)</a></p>

<h3 id="fusionnet-이란">FusionNet 이란?</h3>

<p><strong>FusionNet</strong>은 Connectomics(뇌신경 연결지도를 작성하고 분석하는 신경과학의 일종) 데이터에서 신경구조를 Segmentation 할 목적으로 만든 딥러닝 모델입니다.</p>

<p>논문발표일(2016.12.26) 기준에서 최신의 기술인 Semantic Segmentation과 Residual Neural Networks를 사용 한 모델이라고 하네요.</p>

<p>이 논문에서는 FusionNet을 Electron Microscopy (EM) 이미지에서 Cell Membrane과 Cell Body의 Segmentation에 적용하였습니다.</p>

<h3 id="모델의-특징">모델의 특징</h3>

<p>이전에 connectomics에서 automatic image segmentation을 할 때 보통 <strong>CNN(Convolutional Neural Network)</strong>에 기반 한 <strong>patch-based pixel-wise classification</strong>을 사용했는데 EM 데이터가 용량이 상당히 크기 때문에 처리하기 위한 비용이 많이 들어간다는 문제가 있었습니다.</p>

<p>( 여기서 비용이란 딥러닝을 하기 위한 하드웨어 구입, 전기 사용, 시간 등을 말합니다. )</p>

<p>Encoding, Decoding을 사용한 <strong>FCN(fully Convolution Neural Network)</strong>계열의 모델의 경우 연산량을 줄일 수 있어 비용을 절감 할 수 있는데 이것에 대표적인 모델이 <strong>U-Net</strong>입니다.</p>

<p>하지만 <strong>U-Net</strong>의 방법이 데이터로부터 다중의 맥락 정보를 학습할 수 있지만 <strong>Gradient Vanising</strong> 문제가 발생하기 때문에 깊은 네트워크를 만드는 것은 제한적이었습니다.</p>

<p><strong>FusionNet</strong>은 <strong>residual layers</strong>와 <strong>summation-based skip connection</strong>을 사용하여 <strong>U-Net</strong>의 단점을 보완한 모델입니다.</p>

<p><strong>residual layer</strong></p>

<blockquote>
  <p>이전에 계산된 값을 뒷부분에도 연결시켜서 모델이 깊어져도 값을 잊어버리지 않게 해주는 역할을 함 (vanishing gradient 문제 해결)</p>
</blockquote>

<p><strong>summation-based skip connections</strong></p>

<blockquote>
  <p>층을 건너띄워서 연결, 건너띄워 넘어온 층과 이전의 층을 더해서(합계산) 뒤에 층으로 넘김</p>
</blockquote>

<h5 id="pytorch-code-예시-summation-based-skip-connections">Pytorch Code 예시 (summation-based skip connections)</h5>
<p><img src="../assets/images/posts/2018-05-04-Fusion_Net/fusion-net_1.png" alt="fusion-net_1" /></p>

<h4 id="u-net과-비교하여-가장-특징적인-차이">U-Net과 비교하여 가장 특징적인 차이</h4>

<p><strong>U-net</strong>에서는 긴 skip connection을 통해 feature map을 <strong>이어 붙임 (Concatenating)</strong> 
<strong>FusionNet</strong>에서는 긴 skip connection을 통해 feature map의 <strong>값을 더함</strong> + 짧은 skip connection도 encoding, decoding의 각 step에서 사용</p>

<p>이 방법이 실제로 좋은 퍼포먼스를 보여서 <strong>ISBI 2012 EM segmentation challenge</strong>에서 <strong>1등</strong> 했다고 합니다.
(물론 FusionNet이 100% 기여했다고 까지는 볼 수 없을 것 같습니다.)</p>

<p>OverFitting 해결을 위한 <strong>Data enrichment</strong> (이미지 데이터에 다양한 변형을 가하거나 노이즈를 추가하는 등의 방법을 사용)를 사용 한 것도 좋은 퍼포먼스를 내는데 도움이 되었을 것 입니다.</p>

<p><img src="../assets/images/posts/2018-05-04-Fusion_Net/fusion-net_2.png" alt="fusion-net_2" /></p>

<p>정말 U-Net보다 결과가 좋네요!!!</p>

<h3 id="모델-아키텍처">모델 아키텍처</h3>

<p><img src="../assets/images/posts/2018-05-04-Fusion_Net/fusion-net_3.png" alt="fusion-net_3" /></p>

<p><strong>Encoding Path</strong>	-&gt;	640X640부터 40X40 까지 the features of interest를 검출</p>

<p><strong>Decoding Path</strong>	-&gt;	40X40부터 640X640까지 synthesis를 예측</p>

<p><strong>녹색 블록</strong>	-&gt;	regular convolutional layer, ReLu 활성함수, batch normalization으로 구성</p>

<p><strong>보라색 블록</strong>	-&gt;	3개의 convolutional block으로 구성되는데 마지막 블록3에는 블록1이 블록2와 skip해서 더해진 residual layer가 연결 됨</p>

<p><strong>파란색 블록</strong>	-&gt;	maxpooling layer로써 encoding path에서 feature 압축을 위한 downsampling을 하기 위해 사용</p>

<p><strong>빨간색 블록</strong>	-&gt;	deconvolutional layer로 decoding path에서 interpolation을 사용한 upsampling을 하기 위해 사용</p>

<p><img src="../assets/images/posts/2018-05-04-Fusion_Net/fusion-net_4.png" alt="fusion-net_4" /></p>

<h3 id="참고자료">참고자료</h3>

<p><a href="https://arxiv.org/ftp/arxiv/papers/1612/1612.05360.pdf">FusionNet : A deep fully residual convolutional neural network for image segmentation in connectomics</a></p>

<p>이상으로 FusionNet에 대한 글을 마칩니다. 수정해야 할 내용이 있다면 꼭 말씀 부탁드립니다.
부족한 글이지만 끝까지 읽어주셔서 감사합니다!!!</p>

	  ]]></description>
	</item>

	<item>
	  <title>U-Net</title>
	  <link>http://localhost:20010/U_Net</link>
		
				
		
				
		
				
		
				
		
				
		
				
						<author>강은숙</author>
				
		
				
		
				
		
				
		
	  <pubDate>2018-04-02T09:00:00+09:00</pubDate>
	  <guid>http://localhost:20010/U_Net</guid>
	  <description><![CDATA[
	     <p>논문 링크 : <a href="https://arxiv.org/pdf/1505.04597.pdf">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></p>

<p>이번 블로그의 내용은 Semantic Segmentation의 가장 기본적으로 많이 쓰이는 모델인 U-Net에 대한 내용입니다.</p>

<p>U-Net의 이름은 그 자체로 모델의 형태가 U자로 되어 있어서 생긴 이름입니다.</p>

<p>이번 블로그의 내용을 보시기 전에 앞전에 있는 <a href="https://modulabs-biomedical.github.io/FCN">Fully Convolution for Semantic Segmentation</a> 과 
<a href="https://modulabs-biomedical.github.io/Learning_Deconvolution_Network_for_Semantic_Segmentation">Learning Deconvolution Network for Semantic Segmentation</a> 을 읽고 보시면 더 도움이 되실 것 같습니다!</p>

<h2 id="abstract">Abstract</h2>

<p>Abstract에서는 전체적인 U-Net의 핵심 구조, data augmentation, ISBI challenge에서 좋은 성능을 보였다는 이야기를 하고 있었습니다.</p>

<p>Deep Network는 많은 양의 annotated된 학습 샘플을 가지고 성공적인 training을 해왔습니다.</p>

<p>이 논문에서는 data augmentation을 잘 활용하여 annotated sample을 보다 효율적으로 사용하는 training 전략을 보여줍니다.</p>

<p>논문에서 제안하는 아키텍쳐는 contracting path에서는 context를 캡쳐하고, 대칭적인 구조를 이루는 expanding path에서는 정교한 localization을 가능하게 하는 구조입니다.</p>

<h2 id="1-introduction">1. Introduction</h2>

<p>논문에서 처음에 소개하는 내용은 지난 2년동안 (U-Net은 2015년 5월에 발표되었습니다.) deep convolution network는 많은 visual recognition 작업에서 매우 좋은 성능을 보였지만, training set의 크기와 고려할 네트워크의 크기 때문에 그 성공은 제한적이었다고 말하고 있습니다.</p>

<p>Convolution네트워크의 일반적인 용도는 이미지에 대한 출력이 단일 클래스 레이블인 분류 작업에 있지만, 
많은 시각 작업, 특히 biomedical processing에서 원하는 출력은 localization을 포함해야 하며, 즉 클래스 라벨은 각 pixel에 할당 되어야 한다고 합니다.</p>

<p>U-Net에서 핵심으로 말하고 있는 내용은 세가지로 생각됩니다.</p>

<ol>
  <li>Convolution Encoder에 해당하는 Contracting Path + Convolution Decoder에 해당하는 Expanding Path의 구조로 구성. (해당 구조는 Fully Convolution + Deconvolution 구조의 조합)</li>
  <li>Expanding Path에서 Upsampling 할 때, 좀 더 정확한 Localization을 하기 위해서 Contracting Path의 Feature를 Copy and Crop하여 Concat 하는 구조.</li>
  <li>Data Augmentation</li>
</ol>

<p>기존에는(<a href="http://people.idsia.ch/~juergen/nips2012.pdf">Ciresan et al. [1]</a>) Sliding-window을 하면서 로컬 영역(패치)을 입력으로 제공해서 각 픽셀의 클래스 레이블을 예측했지만, 이 방법은 2가지 단점으로 인해서 Fully Convolution Network구조를 제안하고 있습니다.</p>

<p>두가지 단점은,</p>

<ol>
  <li>네트워크가 각 패치에 대해 개별적으로 실행되어야 하고 패치가 겹쳐 중복성이 많기 때문에 상당히 느리다.</li>
  <li>localization과 context사이에는 trade-off가 있는데, 이는 큰 사이즈의 patches는 많은 max-pooling을 해야해서 localization의 정확도가 떨어질 수 있고, 반면 작은 사이즈의 patches는 협소한 context만을 볼 수 있기 때문입니다.</li>
</ol>

<p>Contracting Path에서 Pooling되기 전의 Feture들은 Upsampling 시에 Layer와 결합되어 고 해상도 output을 만들어 낼 수 있습니다.</p>

<p>하나 더 중요한 점은! 
많은 수의 Feature Channels를 사용하는데요.
아래 네트워크 아키텍쳐를 보시면 DownSampling시에는 64 채널 -&gt; 1024채널까지 증가 되고,
UpSampling시에는 1024 채널 -&gt; 64채널을 사용하고 있습니다.</p>

<p>네트워크는 fully connected layers를 전혀 사용하지 않고, 각 layer에서 convolution만 사용합니다.</p>

<p>다음으로, U-Net에서는 Segmentation시 overlab-tile 전략을 사용합니다. (그림.2)</p>

<p><img src="../assets/images/posts/2018-04-02-U_Net/u-net_fig_2.png" alt="u-net_fig_2" /></p>

<p>이 전략은, 전체 U-Net 아키텍쳐에서 input[572X572] -&gt; output[388X388] 로 Segmentation map이 만들어 질때, 위의 그림에서 보는 것과 같이 border부분에서 정보가 없는 빈 부분이 발생하게 되는데요.</p>

<p><img src="../assets/images/posts/2018-04-02-U_Net/u-net_fig_1_ex.png" alt="u-net_fig_1_ex" /></p>

<p>이 부분을 채우기 위해서 0으로 채우거나, 주변의 값들로 채우거나 이런 방법이 아닌 Mirroring 방법으로 pixel의 값을 채워주는 방법 입니다.</p>

<p>노랑색 영역이 실제 세그멘테이션 될 영역이고, 파랑색 부분이 Patch 입니다.</p>

<p>그림을 확대해서 자세히 보시면, 거울처럼 반사되어 border부분이 채워진 것을 확인 할 수 있었습니다.</p>

<p><img src="../assets/images/posts/2018-04-02-U_Net/u-net_fig_2_ex.png" alt="u-net_fig_2_ex" /></p>

<p>Overlap-tile 이라는 이름은, 파랑색 부분이 Patch단위로 잘라서 세그멘테이션을 하게 되는데 (용어는, Patch == Tile) 이 부분이 아래 그림처럼 겹쳐서 뜯어내서 학습시키기 때문인 것 같습니다.</p>

<p><img src="../assets/images/posts/2018-04-02-U_Net/u-net_fig_2_overlap.png" alt="u-net_fig_2_overlap" /></p>

<h2 id="2-network-architecture">2. Network Architecture</h2>

<p><img src="../assets/images/posts/2018-04-02-U_Net/u-net_fig_1.png" alt="u-net_fig_1" /></p>

<p>Contracting Path는</p>

<ol>
  <li>전형적인 Convolution network 이고,</li>
  <li>두번의 3X3 convolution을 반복 수행하며 (unpadded convolution를 사용),</li>
  <li>ReLU를 사용합니다</li>
  <li>2X2 max pooling 과 stride 2를 사용함</li>
  <li>downsampling시에는 2배의 feture channel을 사용하고</li>
</ol>

<p>Expanding Path는</p>

<ol>
  <li>2X2 convolution (up-convolution)을 사용하고,</li>
  <li>feature channel은 반으로 줄여 사용합니다.</li>
  <li>Contracting Path에서 Max-Pooling 되기 전의 feature map을 Crop 하여 Up-Convolution 할 때 concatenation을 합니다.</li>
  <li>두번의 3X3 convolution 반복하며</li>
  <li>ReLU를 사용합니다</li>
</ol>

<p>마지막 Final Layer에서는 1X1 convolution을 사용하여 2개의 클래스로 분류합니다.</p>

<p>U-Net은 총 23개의 convolution layer가 사용됐습니다.</p>

<h2 id="3-training">3. Training</h2>

<p>학습은 Stochastic gradient descent 로 구현되었습니다.</p>

<p>이 논문에서는 학습시에 GPU memory의 사용량을 최대화 시키기 위해서 batch size를 크게해서 학습시키는 것 보다 input tile 의 size를 크게 주는 방법을 사용하는데요, 
이 방법으로 Batch Size가 작기 때문에, 이를 보완하고자 momentum의 값을 0.99값을 줘서 과거의 값들을 더 많이 반영하게 하여 학습이 더 잘 되도록 하였습니다.</p>

<p><img src="../assets/images/posts/2018-04-02-U_Net/u-net_fig_3.png" alt="u-net_fig_3" /></p>

<h4 id="softmax">softmax</h4>

<p><img src="../assets/images/posts/2018-04-02-U_Net/u-net_softmax.png" alt="softmax" /></p>

<h4 id="cross-entropy-loss-with-wx">Cross Entropy Loss with w(x)</h4>

<p>각각 정답 픽셀에대한 cross entropy loss에는 <script type="math/tex">w(\mathbf x)</script>라는 가중치 값이 추가됩니다. 
여기서  <script type="math/tex">p_{l(x)}(x)</script>의 <script type="math/tex">l(x)</script>는 정답 클래스 즉 위 <strong>softmax 수식</strong>에서 정답의 레이블에 해당하는  <script type="math/tex">k</script> 값을 반환하는 함수 입니다.</p>

<p>cross entropy 함수는 정답의 추정값을 log에 사용하기 때문에 이에 해당하는 정답의 확률을 가져오는 것이죠. 
수식 (1)은 loss 값에 가중치 <script type="math/tex">w(\mathbf x)</script>를 곱한 형태이며 이제 우리가 살펴볼 것은  <script type="math/tex">w(\mathbf x)</script>입니다.</p>

<p><img src="../assets/images/posts/2018-04-02-U_Net/u-net_cross_entropy.png" alt="1_cross_entropy" /></p>

<p><script type="math/tex">w(\mathbf x)</script> <strong>구하는 법 :</strong></p>

<p><img src="../assets/images/posts/2018-04-02-U_Net/u-net_wx.png" alt="2_wx" /></p>

<p><script type="math/tex">w(\mathbf x)</script>는 두개의 텀의 합으로 구성됩니다. <script type="math/tex">w(\mathbf x)</script>는 <script type="math/tex">x</script> 위치의 픽셀에 가중치를 부여하는 함수입니다.</p>

<p><script type="math/tex">w_c(\mathbf x)</script>는  <script type="math/tex">\mathbf x</script> 의 위치에 해당하는 클래스의 빈도수에 따라 값이 결정됩니다.</p>

<p>즉 학습데이터에서 <script type="math/tex">\mathbf x</script> 픽셀이 background일 경우가 많은지 foreground일 경우가 많은지의 빈도수에 따라 결정된다고 보시면 됩니다.</p>

<p>그 뒤의 exp 텀은 <script type="math/tex">d_1</script>, <script type="math/tex">d_2</script> 함수를 포함하는데 <script type="math/tex">d_1</script> 은 <script type="math/tex">\mathbf x</script> 에서 가장 가까운 세포까지의 거리이고 <script type="math/tex">d_2</script>는 두번째로 가까운 세포까지의 거리를 계산하는 함수입니다.</p>

<p>즉  <script type="math/tex">\mathbf x</script>는 세포사이에 존재하는 픽셀이며 두 세포사이의 간격이 좁을 수록 weight를 큰 값으로 두 세포 사이가 넓을 수록 weight를 작은 값으로 갖게 됩니다. 이는 그림 3(d) 를 보시면 명확하게 확인하실 수 있습니다.</p>

<p>네트워크 파라미터의 초기화는 He 초기화 방법을 적용하였습니다.</p>

<h3 id="31-data-augmentation">3.1 Data Augmentation</h3>

<p>Data Augmentation은 3 by 3 elastic 변환 행렬을 통해 수행합니다. <a href="https://en.wikipedia.org/wiki/Transformation_matrix">영상변환 매트릭스</a>는 클릭하셔서 위키의 내용을 참조하면 자세한 내용을 확인하실 수 있습니다. 세포를 세그멘테이션 하는 것이기 때문에 elastic deformation의 적용이 성능향상에 매우 큰 역할을 했다고 합니다.</p>

<h2 id="4-experiments">4. Experiments</h2>

<p>학습한 이후 성능 지표는 EM Segmentation challenge에서 wraping Error / Rand Error / Pixel Error로 1위를 한 지표를 볼 수 있었습니다.</p>

<p><img src="../assets/images/posts/2018-04-02-U_Net/u-net_table1.png" alt="Table1" /></p>

<p>IOU(Intersection over union) 방법으로 측정한 결과로는 아래와 같이 92% / 77.5%로 가장 좋은 성능을 보인것을 확인 할 수 있었습니다.</p>

<p><img src="../assets/images/posts/2018-04-02-U_Net/u-net_table2.png" alt="Table2" /></p>

<h2 id="5-conclusion">5. Conclusion</h2>

<p>마지막으로 결론입니다.</p>

<p>U-Net 구조는 매우 다른 biomedical segmentation applications에서 좋은 성능을 보였고, 
이 성능을 보일 수 있었던 것은 Elastic 변환을 적용한 data augmentation 덕분이고, 
이것은 annotated image가 별로 없는 상황에서 매우 합리적이었다고 합니다.</p>

<p>학습하는데 걸렸던 시간은 NVidia Titan GPU(6GB)를 사용했을때, 10시간이었습니다.</p>

<p>이 U-Net 구현은 Caffe 기반으로 제공되고 있으며, U-Net의 아키텍쳐는 다양한 task에서 쉽게 적용되서 사용될 것을 확신한다고 하면서 논문은 마무리 됩니다.</p>

<p>Image Segmentation Task에서 가장 많이 쓰이는 U-Net은 U자형 아케텍쳐와 Fully Convolution &amp; Deconvolution 구조를 가지고 있는 것으로 좀 더 정확한 Localization을 위해서 Contracting Path의 Feature를 Copy and Crop해서 Expanding Path와 Concat하여 Upsampling을 한다는 것을 확실하게 알아두면 좋을 것 같습니다.</p>

<p>이상 부족하지만, U-Net 논문에 대한 포스팅을 마치겠습니다. 틀린 부분이 있거나 보충 내용이 있으시면 언제든지 말씀해 주시면 수정하도록 하겠습니다!</p>

<p>감사합니다.</p>

	  ]]></description>
	</item>

	<item>
	  <title>Learning Deconvolution Network for Semantic Segmentation</title>
	  <link>http://localhost:20010/Learning_Deconvolution_Network_for_Semantic_Segmentation</link>
		
				
		
				
						<author>오상준</author>
				
		
				
		
				
		
				
		
				
		
				
		
				
		
				
		
	  <pubDate>2018-01-03T09:00:00+09:00</pubDate>
	  <guid>http://localhost:20010/Learning_Deconvolution_Network_for_Semantic_Segmentation</guid>
	  <description><![CDATA[
	     <p><a href="http://arxiv.org/abs/1505.04366">Noh, H., Hong, S., and Han, B. Learning Deconvolution Network for Semantic Segmentation. ICCV, 2015.</a></p>

<p>이번 논문은 <a href="https://modulabs-biomedical.github.io/FCN">앞서 다뤘던 Fully Convolutional Networks</a>와 같은 년도(2015)에 다른 학회(FCN은 CVPR, 본 논문은 ICCV)에 발표된 논문입니다. FCN이나 이후에 다룰 UNet보다는 다소 인기가 적었지만, FCN이 가진 한계를 잘 짚어주셨다는 점에서 공부에 도움이 되었습니다.</p>

<h2 id="fcn의-문제점">FCN의 문제점</h2>

<h3 id="크기에-약하다">크기에 약하다</h3>

<p><img src="assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig1.jpg" alt="" /></p>

<p>위의 예시들처럼 FCN의 추론 결과를 보면, 대상 물체가 너무 큰 경우(a)에는 파편화되고, 너무 작은 경우(b)에는 배경으로 무시되는 경향이 있습니다. FCN에서는 receptive field(상위 레이어의 한 지점에서 참조하는 하위 레이어의 영역)의 크기가 고정되어, 단일 배율(scale)만을 학습하는 것이 이 문제의 원인이라고 본 논문은 지적합니다. 여러 레이어의 결과를 조합하는 skip 구조가 이러한 현상을 완화시켜주기는 하지만, 근본적인 해법은 아니라는 주장입니다.</p>

<h3 id="디테일에-약하다">디테일에 약하다</h3>

<p><img src="assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig5.jpg" alt="" /></p>

<p>FCN이 비록 기존 기법들에 비해 큰 발전을 이루었지만, 세부적인 영역을 찾아내는 데에서는 아직 개선의 여지가 있다고 이 논문은 보고 있습니다. FCN에서는 deconvolution에 들어가는 입력부터 이미 세부 묘사가 떨어지고, deconvolution 과정 자체도 충분히 깊지 않고 너무 단순하다고 말합니다.</p>

<h2 id="논문의-해법">논문의 해법</h2>

<h3 id="네트워크-구조">네트워크 구조</h3>

<p><img src="assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig2.jpg" alt="" /></p>

<p>부족하면 더 넣으면 됩니다. FCN에서는 CNN의 결과를 입력 이미지의 원래 차원으로 확대(upsampling)하는 데에 deconvolution을 사용했지만, 이 논문에서는 deconvolution 시 차원을 유지하는 방법으로, CNN(논문에서 사용한 건 VGG-16)의 convolution만큼 레이어 숫자를 늘렸습니다. 결과적으로 거울에 비춘 모양이 되었습니다.</p>

<p><img src="assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig3.jpg" alt="" /></p>

<p>CNN으로 인해 원래 이미지보다 축소된 차원 크기는 unpooling으로 복원합니다. 여기서 unpooling이란 CNN의 max pooling 시의 위치 정보를 기억했다가, 원래 위치로 그대로 복원해주는 작업입니다.</p>

<p><img src="assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig4.jpg" alt="" /></p>

<p>그 효과는 위의 그림과 같습니다. (b)에서 (c)로 갈 때의 unpooling에 의해, 해상도가 커지는 대신 신호가 흩어져서 희소(sparse)해집니다. 이것을 (c)에서 (d)로 deconvolution을 거치면, 디테일을 살려내면서 신호가 고르게 밀집(dense)됩니다. 이러한 과정이 반복되자 노이즈도 점차 자연스럽게 사라지는 것을 볼 수 있습니다.</p>

<h3 id="학습-및-추론-방식">학습 및 추론 방식</h3>

<p><img src="assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/edge-box.jpg" alt="" /></p>

<p>단일 데이터셋에서 다양한 크기의 사례들을 학습하기 위해, 논문에서는 <a href="">edge-box</a>라는 object proposal 알고리즘을 사용하여 무언가 있을만한 영역을 다양한 크기의 상자로 골라냅니다. 학습 시에는 우선 실제 정답이 가운데에 들어가도록 잘라낸(crop) 이미지들로 1차 학습을, 그 다음 edge-box의 결과물 중 실제 정답과 잘 겹치는 것들을 활용하여 조금 더 심도있는 2차 학습을 진행합니다.</p>

<p><img src="assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig6.jpg" alt="" /></p>

<p>이렇게 학습에 사용된 edge-box는 추론 시에도 사용되는데, 추론 시 사용하는 object proposal의 수(상자 수)를 증가시킬 수록 성능은 좋아진다고 합니다. 물론 그만큼 계산량과 시간은 늘어납니다.</p>

<h2 id="결과">결과</h2>

<p><img src="assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig7.jpg" alt="" /></p>

<p>이렇게 세심하게 설계되고 학습된 결과는 FCN이 실수하는 물체들도 보다 세밀하게 잘 찾아내는 모습을 보입니다. 다만 FCN이 잘 맞추는 곳에서 실수를 할 때도 있는데, 결국 둘을 앙상블하여 conditional random field로 후처리하면 두 가지 모델을 모두 뛰어넘게 되어, FCN과 상호 보완적인 관계에 있다고 논문은 맺습니다.</p>

<h2 id="추가-참고-문헌">추가 참고 문헌</h2>

<ul>
  <li><a href="http://www.matthewzeiler.com/wp-content/uploads/2017/07/cvpr2010.pdf">Zeiler, M. D. et al. Deconvolutional Networks. CVPR, 2010.</a></li>
  <li><a href="https://www.microsoft.com/en-us/research/publication/edge-boxes-locating-object-proposals-from-edges/">Zitnick, L. and Dollar, P. Edge Boxes: Locating Object Proposals from Edges. ECCV, 2014.</a></li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Fully Convolutional Networks for Semantic Segmentation</title>
	  <link>http://localhost:20010/FCN</link>
		
				
						<author>김승일</author>
				
		
				
		
				
		
				
		
				
		
				
		
				
		
				
		
				
		
	  <pubDate>2017-12-21T19:00:00+09:00</pubDate>
	  <guid>http://localhost:20010/FCN</guid>
	  <description><![CDATA[
	     <p>논문 링크 : <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf">Fully Convolutional Networks for Semantic Segmentation</a></p>

<h2 id="introduction">Introduction</h2>

<p>Semantic Segmentation는 영상을 pixel단위로 어떤 object인지 classification 하는 것이라고 볼 수 있습니다. (언제나 강력추천하는) <a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf">cs231n 강의 자료</a>를 보시면 쉽게 잘 나와 있죠.</p>

<p><img src="assets/images/posts/2017-12-21-FCN/fig1.jpg" alt="" />
Figure 1. Computer Vision Tasks</p>

<p>예전에는 이렇게 pixel 단위로 classification을 하기 위해,</p>

<ol>
  <li>그림 2와 같이 일정 영역을 포함하는 window를 만들고,</li>
  <li>window 내 영상의 object를 classifiy해서</li>
  <li>window 중앙의 pixel의 class 값이라고 간주하는</li>
</ol>

<p>방식을 주로 사용했습니다.</p>

<p><img src="assets/images/posts/2017-12-21-FCN/fig2.jpg" alt="" />
Figure 2.</p>
<strike> 그림이 필요해서 커피숖에서 작업중에 급조를...</strike>

<p>당연히 
<strong>계산량의 문제</strong> 와 global information을 사용하지 못하고 window라는 제한된 영역의 <strong>local information만 사용한다</strong> 는 문제가 있을 겁니다.</p>

<p>이 논문에서는 
local feature와 global feature를 모두 사용하고
계산량 잡아먹는 주범인 fully connected layer를 없앤 CNN architecture를 제안합니다.</p>

<hr />

<h2 id="network-architecture">Network Architecture</h2>
<p>Fully Convolutional Networks의 구조는 다음 그림 3과 같으며, 크게 4가지 부분으로 구성이 된다고 볼 수 있습니다.</p>

<p><img src="assets/images/posts/2017-12-21-FCN/fig3.jpg" alt="" />
Figure 3. Architecture of Fully Convolutional Networks</p>

<blockquote>
  <p><strong>네트워크 구조</strong></p>
</blockquote>

<blockquote>
  <p><strong>(1) Feature Extraction</strong> : 
 일반적인 CNN의 구조에서 많이 보이는 conv layer들로 구성되어 있습니다. 
 <strong>(2) Feature-level Classification</strong> : 
 추출된 Feature map의 pixel 하나하나마다 classification을 수행합니다. 이 때 classification된 결과는 매우 coarse합니다. (그림 3에서 초록색 박스에 tabby cat class에 대한 Classification 결과 참고) 
 <strong>(3) Upsampling</strong> :
 coarse 한 결과를 backward strided convolution 을 통해 upsampling하여 원래의 image size로 키워줍니다. 
<strong>(4) Segmentation</strong> : 
각 class의 upsampling된 결과를 사용하여 하나의 Segmentation  결과 이미지를 만들어 줍니다.</p>
</blockquote>

<p>그러면, 각 네트워크의 내부들을 한번 살펴보겠습니다.</p>

<p>####<em>Feature Extraction</em>
<a href="https://arxiv.org/abs/1409.1556">VGG-19 network</a>를 feature extractor로 사용한다고 가정을 해봅시다. 이 경우 conv1 ~ conv5 layer ( or pool5 layer) 까지 통과하면서 feature를 추출합니다. 낮은 layer의 경우 작은 receptive field를 지니므로 작은 크기의 feature가, 높은 layer의 경우 높은 receptive field를 지니므로 큰 크기의 feature가 추출되게 되죠.
이렇게 추출된 최종 feature map (conv5 or pool5 layer)을 이용하여 다음 단계에서 coarse한 segmentation map 을 만들어냅니다.</p>

<p><img src="assets/images/posts/2017-12-21-FCN/fig4.jpg" alt="" />
Figure 4. Feature Extraction on VGG-19 Networks</p>

<h4 id="feature-level-classification"><em>Feature-level Classification</em></h4>

<p>원래 VGG network는 이렇게 추출된 feature의 뒤에 4096, 4096, 1000으로 이어지는 fully connected layer를 연결하여 classification을 합니다만(그림 4), 본 논문에서는 이런 fully connected layer를 없애버립니다. 그리고, 1x1 conv layer를 추가합니다. 그림 3에 보시면 4096, 4096, 21 이라고 표현된 1x1 conv layer를 보실 수 있는데요. 
(1000이 21로 바뀐 이유는 이 논문에서는 PASCAL VOC dataset으로 실험을 하는데, 그 데이터의 클래스가 20개 + background 이기 때문입니다.)
이 1x1 conv 의 결과물이 결국 각 class의 feature map 상에서의 classifiation (즉, segmentation) 이 됩니다. 그림 3에 보면 conv8(마지막 1x1 conv) layer의 depth channel 중에서 tabby cat에 해당하는 class의 feature map  상에서의 classification (즉, segmentation) 결과 heatmap 을 볼 수 있습니다. 마지막 1x1 conv layer에서 depth channel은 각 class를 의미하므로, 어떤 class의 segmentation heatmap도 추정할 수 있습니다.</p>

<h4 id="upsampling"><em>Upsampling</em></h4>

<p>그런데 feature map level에서 segmentation 한 결과는 너무 coarse한 결과입니다. (그림 3의 tabby cat heatmap 보면 깍두기처럼…) 따라서, 이 coarse한 heatmap을 dense하게 (원래의 image size로) 만들어주어야 합니다. 본 논문에서는 upsampling(backwards strided convolution)을 사용합니다. 
그러면 각 class별로 dense한 segmentation 결과를 얻을 수 있습니다. 즉, 원래 image의 폭을 W, 높이를 H, 라고 한다면, WxHx21 의 dense heatmap결과를 얻을 수 있습니다.</p>

<h4 id="segmentation"><em>Segmentation</em></h4>
<p>그러나 우리는 결국 각 class 별 결과를 추정하고자 하는 것이 아니죠. 하나의 이미지에서 모든 class의 segmentation된 결과를 얻어야 합니다. 그래서 윗 단계에서 얻어진 upsampling된 각 class별 heatmap을 softmax를 이용하여 가장 높은 확률을 가지는 class만 모아서 한장의 segmentation 이미지로 만듭니다.</p>

<hr />

<h2 id="skip-combining">Skip Combining</h2>
<p>그런데 3단계의 <strong><em>Upsampling</em></strong> 과정에서 coarse한 결과를 dense하게 만들어줄 때 너무 많이 뻥튀기를 하기 때문에 detail아 다 뭉개진 segmenation 결과를 얻을 수 밖에 없습니다. (그림 5 참고)</p>

<p><img src="assets/images/posts/2017-12-21-FCN/fig5.jpg" alt="" />
Figure 5. Segmentation Result from the Last Conv Layer</p>

<p>다음 그림 6과 같은 CNN 구조의 Fully Convolutional Networks가 있다고 가정해봅시다. 최종 결과물인 FCN-32s는 32배로 upsampling을 하기 때문에 detail이 많이 사라진 segmentation 결과를 보여줍니다.</p>

<p><img src="assets/images/posts/2017-12-21-FCN/fig6.jpg" alt="" />
Figure 6. (Conventional) Fully Convolutional Networks : FCN-32s</p>

<p>본 논문에서는 이 문제를 해결하기 위해 그 이전 layer의 feature map을 이용하는 skip combining 기법을 사용합니다(그림 7 참고).</p>

<p><img src="assets/images/posts/2017-12-21-FCN/fig7.jpg" alt="" />
Figure 7. Fully Convolutional Networks with Skip Combining : FCN-16s</p>

<p>그림 6에서는 마지막 conv layer인 conv 7에서 32배 upsampling하여 segmentation 결과를 만들었습니다. 하지만, 그림 7에서는 마지막 conv layer 결과를 2배 upsampling 하고 마지막 pooling layer (pool5)이전 단계 즉, pool 4 layer의 결과와 합쳐줍니다. 그리고 난 후, 그 합쳐진 결과를 16배 upsampling 하여 FCN-16s 라는 segmentation 결과 이미지를 만들어 냅니다.</p>

<p>여기서 합친다는 말은 그냥 <strong>더해준다</strong>는 의미입니다. 
<a href="https://github.com/shekkizh/FCN.tensorflow/blob/master/FCN.py">다음 코드</a>를 참고하세요.</p>

<p><img src="assets/images/posts/2017-12-21-FCN/fig7.code.jpg" alt="" /></p>

<p>그렇다면 하나 더 이전의 pooling layer와도 합칠 수 있지 않을까요? 당연히 있습니다.
그림 8은 pool 3 layer + 2배 upsampling된 pool 4 layer + 4배 upsampling 된 conv7 layer 값을 다시 8배 upsampling 하여 FCN-8s라는 보다 detail한 segmentation 이미지를 만들어내는 구조입니다.</p>

<p><img src="assets/images/posts/2017-12-21-FCN/fig8.jpg" alt="" />
Figure 8. Fully Convolutional Networks with Skip Combining : FCN-8s</p>

<p>자 그러면 각 skip combining 한 후의 최종 segmentation 결과를 살펴볼까요? 확실히 FCN-32s에 비해 FCN-16s가, FCN-16s에 비해 FCN-8s가 detail한 segmentation 결과를 보여줌을 알 수 있습니다.</p>

<p><img src="assets/images/posts/2017-12-21-FCN/fig9.jpg" alt="" />
Figure 9. Segmentation Results</p>


	  ]]></description>
	</item>


</channel>
</rss>

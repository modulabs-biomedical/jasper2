<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-08-20T16:16:40+09:00</updated><id>http://localhost:4000/</id><title type="html">바이오메디컬랩@모두의연구소</title><subtitle>모두의연구소 바이오메디컬랩 기술 블로그입니다.</subtitle><entry><title type="html">Tensorpack</title><link href="http://localhost:4000/TensorPack" rel="alternate" type="text/html" title="Tensorpack" /><published>2018-08-17T09:00:00+09:00</published><updated>2018-08-17T09:00:00+09:00</updated><id>http://localhost:4000/TensorPack</id><content type="html" xml:base="http://localhost:4000/TensorPack">&lt;h2 id=&quot;tensorpack-구조-이해하기&quot;&gt;Tensorpack 구조 이해하기&lt;/h2&gt;

&lt;h6 id=&quot;modeldesc와-trainer를-중심으로&quot;&gt;ModelDesc와 Trainer를 중심으로&lt;/h6&gt;

&lt;h6 id=&quot;peter-cha&quot;&gt;Peter Cha&lt;/h6&gt;

&lt;h4 id=&quot;tensorpack을-공부하면-&quot;&gt;Tensorpack을 공부하면, 😐&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;우선 모르는 것들 투성이다. 알고나면 너무 쓰기 편하지만, 처음 접할 때는 너무 많이 추상화 된 API에 ‘이게 tensorflow는 맞는지..’할 정도니까.&lt;/li&gt;
  &lt;li&gt;우선 이 튜토리얼을 보기 전, 필자의 &lt;a href=&quot;https://github.com/PeterCha90/Tensorflow-Deep-Learning/blob/master/Tensorpack_tutorial.ipynb&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorpack_tutorial.ipynb&lt;/code&gt;&lt;/a&gt;를 먼저 보길 바란다. 대략적인 dataflow는 데이터를 불러오는 부분으로 이해를 마쳤다고 생각하고, dataflow부분은 생략하고 설명을 진행하도록 하겠다.&lt;/li&gt;
  &lt;li&gt;이번에는 Model의 선언하게 될 때 상속받은 &lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ModelDesc&lt;/code&gt;&lt;/strong&gt; class와, 학습을 실행하는 Trainer들의 모태가 되는 &lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TowerTrainer&lt;/code&gt;&lt;/strong&gt; 에 대해 알아보고자 한다.&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorpack_tutorial.ipynb&lt;/code&gt;에서 설명에 해당하는 부분을 함께 찾아보면 이해에 도움이 더 될 것 같다.&lt;/li&gt;
  &lt;li&gt;이 Tutorial은 Tensorpack &lt;a href=&quot;https://tensorpack.readthedocs.io/modules/train.html?highlight=TowerFuncWrapper&quot;&gt;documentation&lt;/a&gt;을 참고해서 만들었다.
    &lt;hr /&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-class-modeldescbase-&quot;&gt;1. Class &lt;code class=&quot;highlighter-rouge&quot;&gt;ModelDescBase&lt;/code&gt; 😏&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Base class for a model description이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ModelDesc&lt;/code&gt;는 ModelDescBase를 기반으로 만들어졌기 때문에, &lt;code class=&quot;highlighter-rouge&quot;&gt;ModelDescBase&lt;/code&gt;를 먼저 설명한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;11-build_graphargs&quot;&gt;1.1. build_graph(*args)&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;모든 symbolic graph (&lt;strong&gt;Model의 형태&lt;/strong&gt;)를 Build한다. 이 함수가 뒤에서 설명할 &lt;code class=&quot;highlighter-rouge&quot;&gt;TowerTrainer&lt;/code&gt;에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;tower function&lt;/code&gt;의 일부분이다.&lt;/li&gt;
  &lt;li&gt;그 다음 설명할 &lt;code class=&quot;highlighter-rouge&quot;&gt;inputs()&lt;/code&gt;에서 정의된 input list에 맞는 &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Tensor&lt;/code&gt;를 parameter로 받는다.&lt;/li&gt;
  &lt;li&gt;아무것도 리턴하지 않는다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;12-inputs&quot;&gt;1.2. inputs()&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Model에서 input으로 받을 텐서들의 placeholder들을 정의하는 함수다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;후에 &lt;code class=&quot;highlighter-rouge&quot;&gt;InputDesc&lt;/code&gt;로 변환될, &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.placeholder&lt;/code&gt;들을 return 한다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-08-17-TensorPack/modeldesc.png&quot; alt=&quot;u-net_fig_2&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;13-get_inputs_desc&quot;&gt;1.3. get_inputs_desc&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;이름에서 알 수 있듯이, inputs()에서 정의된 모양대로 생긴 InputDesc를 list로 반환하는 함수다.
    &lt;hr /&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-class-modeldesc-&quot;&gt;2. Class &lt;code class=&quot;highlighter-rouge&quot;&gt;ModelDesc&lt;/code&gt; 😎&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;주의사항&lt;/strong&gt;: &lt;strong&gt;build_graph()를 꼭 cost를 return하도록&lt;/strong&gt; 코딩해야 한다.&lt;/li&gt;
  &lt;li&gt;앞에서 설명한 ModelDescBase를 상속받은 터라, 위의 3가지는 함수는 내장하고 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;21-optimizer&quot;&gt;2.1. optimizer()&lt;/h6&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tf.train.Optimizer&lt;/code&gt;를 여기에 선언해주고 Return하게끔 함수를 짜준다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;22-get_optimizer&quot;&gt;2.2. get_optimizer()&lt;/h6&gt;
&lt;ul&gt;
  &lt;li&gt;optimizer()를 호출하면, 계속 새로 optimizer를 만들어서 생성하는데, 이 함수를 쓰면 이미 optimizer()를 통해 생긴 optimizer를 기록해 놓았다가 반환시켜준다.
    &lt;hr /&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-class-towertrainer-&quot;&gt;3. Class &lt;code class=&quot;highlighter-rouge&quot;&gt;TowerTrainer&lt;/code&gt; 😶&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Tensorpack에서는, 우리가 흔히 말하는 Model을 계속 Tower라고 지칭한다.(왜 그런지 모르겠다. 😶)&lt;/li&gt;
  &lt;li&gt;그래서 아래에서 나오는 &lt;code class=&quot;highlighter-rouge&quot;&gt;TowerTrainer&lt;/code&gt;는 만든 모델을 학습을 시키는 &lt;strong&gt;Trainer&lt;/strong&gt;고, 그 트레이너가 어떤 특징들을 가진 함수들을 들고 다니는지 생각하면 이해가 쉽다.&lt;/li&gt;
  &lt;li&gt;기본적으로 Tensorpack에 나오는 모든 Trainer들은 &lt;code class=&quot;highlighter-rouge&quot;&gt;TowerTrainer&lt;/code&gt;의 subclass다 이 개념이 그래서 궁극적으로는 모든 neural-network training을 가능하게 해준다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;31-get_predictorinput_names-output_names-device0&quot;&gt;3.1. &lt;code class=&quot;highlighter-rouge&quot;&gt;get_predictor&lt;/code&gt;(input_names, output_names, device=0)&lt;/h6&gt;

&lt;blockquote&gt;
  &lt;p&gt;Returns a callable predictor built under &lt;code class=&quot;highlighter-rouge&quot;&gt;TowerContext(is_training=False)&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;이 함수가 호출되면, 가지고 있는 TowerContext(모델의 Train or Test 여부)를 &lt;u&gt;training mode가 아닌 상태(is_training=False, 즉=Test 모드)&lt;/u&gt;로 돌려준다. 그러니까 &lt;strong&gt;Test data로 시험할 때만 부르는 함수&lt;/strong&gt;. 그래서 이름도 predictor.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Parameters&lt;/code&gt;&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;input_names&lt;/code&gt; &lt;strong&gt;(list)&lt;/strong&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;output_names&lt;/code&gt; &lt;strong&gt;(list)&lt;/strong&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;device&lt;/code&gt; &lt;strong&gt;(int)&lt;/strong&gt; – build the predictor on device ‘/gpu:{device}’ or use -1 for ‘/cpu:0’.&lt;/li&gt;
  &lt;li&gt;파라미터로 들어가는 input, output이름은 모델 안에서 선언된 이름이 아니면 안 돌아가니까 조심.&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;32-inputs_desc&quot;&gt;3.2. inputs_desc&lt;/h6&gt;

&lt;blockquote&gt;
  &lt;p&gt;Returns – list[InputDesc]: metainfo about the inputs to the tower.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;말 그대로, 모델에 들어갈 Input의 크기와 같은 정보가 들어있는 list를 반환해준다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;33-tower_func&quot;&gt;3.3. tower_func&lt;/h6&gt;

&lt;blockquote&gt;
  &lt;p&gt;Build Model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;이 친구가 실제 &lt;strong&gt;모델을 정의하고, Build&lt;/strong&gt;할 수 있는 함수를 세팅하는 부분!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ModelDesc&lt;/code&gt;&lt;/strong&gt; interface로 정의된 model을 trainer로 돌려야 하는 상황이 자주 발생할 수 있는데, 이 때, &lt;u&gt;ModelDesc에서 선언된 **build_graph 함수**가 이 역할을 대신&lt;/u&gt;해 줄 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;34-towers&quot;&gt;3.4. towers&lt;/h6&gt;
&lt;blockquote&gt;
  &lt;p&gt;Returns – a TowerTensorHandles object, to
access the tower handles by either indices or names.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;모델 및 Train 전반에 걸쳐 관련된 변수들에 접근하고 싶을 때 사용한다! 그래서 이 함수는 Transfer learning을 할 때 유용할 거 같다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이미 &lt;strong&gt;모델 그래프가 Set up이 끝난 뒤에만&lt;/strong&gt; 이 함수는 호출될 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;각각의 &lt;code class=&quot;highlighter-rouge&quot;&gt;layer&lt;/code&gt;와 &lt;code class=&quot;highlighter-rouge&quot;&gt;attributes&lt;/code&gt;에 이 &lt;code class=&quot;highlighter-rouge&quot;&gt;towers&lt;/code&gt;함수를 호출하면 접근할 수 있게 된다! 아래는 예시.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-08-17-TensorPack/3-4.png&quot; alt=&quot;u-net_fig_2&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;​&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;4-class-trainer-&quot;&gt;4. Class &lt;code class=&quot;highlighter-rouge&quot;&gt;Trainer&lt;/code&gt; 🙄&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Base class for a trainer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;분명히 위에서 금방,
    &lt;blockquote&gt;
      &lt;p&gt;“기본적으로 Tensorpack에 나오는 모든 Trainer들은 &lt;code class=&quot;highlighter-rouge&quot;&gt;TowerTrainer&lt;/code&gt;의 subclass다”&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;라고 했는데, 이 &lt;code class=&quot;highlighter-rouge&quot;&gt;TowerTrainer가 상속을 받는 class&lt;/code&gt;가 있었으니, 이름하여 TowerTrainer보다 더 단순한 &lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Trainer&lt;/code&gt;&lt;/strong&gt; 다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;다른 TowerTrainer를 상속 받은 Trainer들을 사용할 때, 종종 TowerTrainer에서 본 적 없는 친구들이 나타나는데, 그 친구들이 Trainer의 것인 경우가 있다.&lt;/li&gt;
  &lt;li&gt;하지만, Trainer 고유 함수나 요소에 직접적으로 접근할 일이 별로 없어서 아래의 3가지 정도만 알고 있으면 될 것 같다. 나머지는 &lt;a href=&quot;https://tensorpack.readthedocs.io/modules/train.html?highlight=register_callback#tensorpack.train.Trainer&quot;&gt;문서&lt;/a&gt;를 참고하자.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;아래 1, 2번의 max_epoch과, steps_per_epoch은 &lt;code class=&quot;highlighter-rouge&quot;&gt;TrainConfig&lt;/code&gt;에서 자주 만나는 키워드들인데, 이 친구들이 Trainer의 요소였다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h6 id=&quot;41-max_epoch&quot;&gt;4.1. max_epoch&lt;/h6&gt;
&lt;ul&gt;
  &lt;li&gt;Epoch은 몇 번 돌릴 것인지&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;42-steps_per_epoch&quot;&gt;4.2. steps_per_epoch&lt;/h6&gt;
&lt;ul&gt;
  &lt;li&gt;한 에폭당 steps은 총 몇 번인지.
&lt;img src=&quot;../assets/images/posts/2018-08-17-TensorPack/sample.png&quot; alt=&quot;u-net_fig_2&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;43-register_callbackcb&quot;&gt;4.3. register_callback(cb)&lt;/h6&gt;

&lt;blockquote&gt;
  &lt;p&gt;Register callbacks to the trainer. It can only be called before Trainer.train().&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;u&gt;Trainer가 모델을 돌릴 때마다(epoch이 진행 됨에 따라), 수행하게 될 부가적인 기능&lt;/u&gt;들을 Tensorpack에서는 &lt;strong&gt;callback&lt;/strong&gt;이라고 부르고, 대표적인 callback으로 &lt;strong&gt;ModelSaver()&lt;/strong&gt; 가 있다.&lt;/li&gt;
  &lt;li&gt;이 Callback을 명시적으로 전달하여 Trainer Object에 세팅할 수 있는 기능이다. 주로 모델을 튜닝할 때, 설정하면서 종종 쓰는 것을 코드 상에서 확인할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;5-towercontext-&quot;&gt;5. TowerContext 😛&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TowerContext&lt;/code&gt;&lt;/strong&gt; 는 Training과 Validation 혹은 Test시에 동작이 달라야 하는 &lt;code class=&quot;highlighter-rouge&quot;&gt;BatchNorm&lt;/code&gt;이나, &lt;code class=&quot;highlighter-rouge&quot;&gt;Dropout&lt;/code&gt;을 제어하기 위해서 만들어진 function이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorpack_tutorial.ipynb&lt;/code&gt;에서는 이 친구를 찾아볼 수 없는데, SimpleTrainer 소스코드를 보니, 자체적으로 안에서 train/test time에 맞춰서 TrainTowerContext라는 것으로 조절하고 있기 때문이었다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;사용법은 간단하다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-08-17-TensorPack/5-1.png&quot; alt=&quot;u-net_fig_2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;그래서, 내가 세운 모델을 외부에서 사용하고 싶을 때, 즉, 나만의 Trainer를 새로 정의해서 train/test time때, 다르게 동작을 해야하는 상황이라면, TowerContext를 적절히 써서 분기시켜줘야 한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;아래는 Tensorpack Github에서 제공하는 &lt;a href=&quot;https://github.com/tensorpack/tensorpack/blob/master/examples/GAN/GAN.py&quot;&gt;GANTrainer&lt;/a&gt;에서 실제로 TowerContext를 어떻게 설정해주는지 보여주는 예시다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-08-17-TensorPack/5-2.png&quot; alt=&quot;u-net_fig_2&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;​&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;thank-you-&quot;&gt;Thank you 🙇&lt;/h2&gt;</content><author><name>차예솔</name></author><category term="개념 정리" /><summary type="html">Tensorpack 구조 이해하기</summary></entry><entry><title type="html">FusionNet</title><link href="http://localhost:4000/FusionNet" rel="alternate" type="text/html" title="FusionNet" /><published>2018-05-04T09:00:00+09:00</published><updated>2018-05-04T09:00:00+09:00</updated><id>http://localhost:4000/FusionNet</id><content type="html" xml:base="http://localhost:4000/FusionNet">&lt;h1 id=&quot;fusionnet&quot;&gt;&lt;strong&gt;FusionNet&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;FusionNet은 U-Net처럼 Semantic Segmentation에 활용 할 수 있는 모델입니다.&lt;/p&gt;

&lt;p&gt;이름이 FusionNet인 이유는 아마도 Encoder에 있는 Layer를 가져와 Decoder에 결합(Fusion)하는 방법이 이 모델에 가장 특징적인 부분이기 때문인 것 같습니다.&lt;/p&gt;

&lt;p&gt;U-Net과 유사한 부분이 많기 때문에 블로그에 U-Net글을 읽으시면 많은 도움이 될 것 같습니다. 
&lt;a href=&quot;https://modulabs-biomedical.github.io/U_Net&quot;&gt;U-Net (by 강은숙)&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;fusionnet-이란&quot;&gt;FusionNet 이란?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;FusionNet&lt;/strong&gt;은 Connectomics(뇌신경 연결지도를 작성하고 분석하는 신경과학의 일종) 데이터에서 신경구조를 Segmentation 할 목적으로 만든 딥러닝 모델입니다.&lt;/p&gt;

&lt;p&gt;논문발표일(2016.12.26) 기준에서 최신의 기술인 Semantic Segmentation과 Residual Neural Networks를 사용 한 모델이라고 하네요.&lt;/p&gt;

&lt;p&gt;이 논문에서는 FusionNet을 Electron Microscopy (EM) 이미지에서 Cell Membrane과 Cell Body의 Segmentation에 적용하였습니다.&lt;/p&gt;

&lt;h3 id=&quot;모델의-특징&quot;&gt;모델의 특징&lt;/h3&gt;

&lt;p&gt;이전에 connectomics에서 automatic image segmentation을 할 때 보통 &lt;strong&gt;CNN(Convolutional Neural Network)&lt;/strong&gt;에 기반 한 &lt;strong&gt;patch-based pixel-wise classification&lt;/strong&gt;을 사용했는데 EM 데이터가 용량이 상당히 크기 때문에 처리하기 위한 비용이 많이 들어간다는 문제가 있었습니다.&lt;/p&gt;

&lt;p&gt;( 여기서 비용이란 딥러닝을 하기 위한 하드웨어 구입, 전기 사용, 시간 등을 말합니다. )&lt;/p&gt;

&lt;p&gt;Encoding, Decoding을 사용한 &lt;strong&gt;FCN(fully Convolution Neural Network)&lt;/strong&gt;계열의 모델의 경우 연산량을 줄일 수 있어 비용을 절감 할 수 있는데 이것에 대표적인 모델이 &lt;strong&gt;U-Net&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p&gt;하지만 &lt;strong&gt;U-Net&lt;/strong&gt;의 방법이 데이터로부터 다중의 맥락 정보를 학습할 수 있지만 &lt;strong&gt;Gradient Vanising&lt;/strong&gt; 문제가 발생하기 때문에 깊은 네트워크를 만드는 것은 제한적이었습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FusionNet&lt;/strong&gt;은 &lt;strong&gt;residual layers&lt;/strong&gt;와 &lt;strong&gt;summation-based skip connection&lt;/strong&gt;을 사용하여 &lt;strong&gt;U-Net&lt;/strong&gt;의 단점을 보완한 모델입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;residual layer&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;이전에 계산된 값을 뒷부분에도 연결시켜서 모델이 깊어져도 값을 잊어버리지 않게 해주는 역할을 함 (vanishing gradient 문제 해결)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;summation-based skip connections&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;층을 건너띄워서 연결, 건너띄워 넘어온 층과 이전의 층을 더해서(합계산) 뒤에 층으로 넘김&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;pytorch-code-예시-summation-based-skip-connections&quot;&gt;Pytorch Code 예시 (summation-based skip connections)&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-05-04-Fusion_Net/fusion-net_1.png&quot; alt=&quot;fusion-net_1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;u-net과-비교하여-가장-특징적인-차이&quot;&gt;U-Net과 비교하여 가장 특징적인 차이&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;U-net&lt;/strong&gt;에서는 긴 skip connection을 통해 feature map을 &lt;strong&gt;이어 붙임 (Concatenating)&lt;/strong&gt; 
&lt;strong&gt;FusionNet&lt;/strong&gt;에서는 긴 skip connection을 통해 feature map의 &lt;strong&gt;값을 더함&lt;/strong&gt; + 짧은 skip connection도 encoding, decoding의 각 step에서 사용&lt;/p&gt;

&lt;p&gt;이 방법이 실제로 좋은 퍼포먼스를 보여서 &lt;strong&gt;ISBI 2012 EM segmentation challenge&lt;/strong&gt;에서 &lt;strong&gt;1등&lt;/strong&gt; 했다고 합니다.
(물론 FusionNet이 100% 기여했다고 까지는 볼 수 없을 것 같습니다.)&lt;/p&gt;

&lt;p&gt;OverFitting 해결을 위한 &lt;strong&gt;Data enrichment&lt;/strong&gt; (이미지 데이터에 다양한 변형을 가하거나 노이즈를 추가하는 등의 방법을 사용)를 사용 한 것도 좋은 퍼포먼스를 내는데 도움이 되었을 것 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-05-04-Fusion_Net/fusion-net_2.png&quot; alt=&quot;fusion-net_2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;정말 U-Net보다 결과가 좋네요!!!&lt;/p&gt;

&lt;h3 id=&quot;모델-아키텍처&quot;&gt;모델 아키텍처&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-05-04-Fusion_Net/fusion-net_3.png&quot; alt=&quot;fusion-net_3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Encoding Path&lt;/strong&gt;	-&amp;gt;	640X640부터 40X40 까지 the features of interest를 검출&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Decoding Path&lt;/strong&gt;	-&amp;gt;	40X40부터 640X640까지 synthesis를 예측&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;녹색 블록&lt;/strong&gt;	-&amp;gt;	regular convolutional layer, ReLu 활성함수, batch normalization으로 구성&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;보라색 블록&lt;/strong&gt;	-&amp;gt;	3개의 convolutional block으로 구성되는데 마지막 블록3에는 블록1이 블록2와 skip해서 더해진 residual layer가 연결 됨&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;파란색 블록&lt;/strong&gt;	-&amp;gt;	maxpooling layer로써 encoding path에서 feature 압축을 위한 downsampling을 하기 위해 사용&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;빨간색 블록&lt;/strong&gt;	-&amp;gt;	deconvolutional layer로 decoding path에서 interpolation을 사용한 upsampling을 하기 위해 사용&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-05-04-Fusion_Net/fusion-net_4.png&quot; alt=&quot;fusion-net_4&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;참고자료&quot;&gt;참고자료&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/ftp/arxiv/papers/1612/1612.05360.pdf&quot;&gt;FusionNet : A deep fully residual convolutional neural network for image segmentation in connectomics&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이상으로 FusionNet에 대한 글을 마칩니다. 수정해야 할 내용이 있다면 꼭 말씀 부탁드립니다.
부족한 글이지만 끝까지 읽어주셔서 감사합니다!!!&lt;/p&gt;</content><author><name>김경모</name></author><category term="논문 리뷰" /><summary type="html">FusionNet FusionNet은 U-Net처럼 Semantic Segmentation에 활용 할 수 있는 모델입니다. 이름이 FusionNet인 이유는 아마도 Encoder에 있는 Layer를 가져와 Decoder에 결합(Fusion)하는 방법이 이 모델에 가장 특징적인 부분이기 때문인 것 같습니다. U-Net과 유사한 부분이 많기 때문에 블로그에 U-Net글을 읽으시면 많은 도움이 될 것 같습니다. U-Net (by 강은숙) FusionNet 이란? FusionNet은 Connectomics(뇌신경 연결지도를 작성하고 분석하는 신경과학의 일종) 데이터에서 신경구조를 Segmentation 할 목적으로 만든 딥러닝 모델입니다. 논문발표일(2016.12.26) 기준에서 최신의 기술인 Semantic Segmentation과 Residual Neural Networks를 사용 한 모델이라고 하네요. 이 논문에서는 FusionNet을 Electron Microscopy (EM) 이미지에서 Cell Membrane과 Cell Body의 Segmentation에 적용하였습니다. 모델의 특징 이전에 connectomics에서 automatic image segmentation을 할 때 보통 CNN(Convolutional Neural Network)에 기반 한 patch-based pixel-wise classification을 사용했는데 EM 데이터가 용량이 상당히 크기 때문에 처리하기 위한 비용이 많이 들어간다는 문제가 있었습니다. ( 여기서 비용이란 딥러닝을 하기 위한 하드웨어 구입, 전기 사용, 시간 등을 말합니다. ) Encoding, Decoding을 사용한 FCN(fully Convolution Neural Network)계열의 모델의 경우 연산량을 줄일 수 있어 비용을 절감 할 수 있는데 이것에 대표적인 모델이 U-Net입니다. 하지만 U-Net의 방법이 데이터로부터 다중의 맥락 정보를 학습할 수 있지만 Gradient Vanising 문제가 발생하기 때문에 깊은 네트워크를 만드는 것은 제한적이었습니다. FusionNet은 residual layers와 summation-based skip connection을 사용하여 U-Net의 단점을 보완한 모델입니다. residual layer 이전에 계산된 값을 뒷부분에도 연결시켜서 모델이 깊어져도 값을 잊어버리지 않게 해주는 역할을 함 (vanishing gradient 문제 해결) summation-based skip connections 층을 건너띄워서 연결, 건너띄워 넘어온 층과 이전의 층을 더해서(합계산) 뒤에 층으로 넘김 Pytorch Code 예시 (summation-based skip connections) U-Net과 비교하여 가장 특징적인 차이 U-net에서는 긴 skip connection을 통해 feature map을 이어 붙임 (Concatenating) FusionNet에서는 긴 skip connection을 통해 feature map의 값을 더함 + 짧은 skip connection도 encoding, decoding의 각 step에서 사용 이 방법이 실제로 좋은 퍼포먼스를 보여서 ISBI 2012 EM segmentation challenge에서 1등 했다고 합니다. (물론 FusionNet이 100% 기여했다고 까지는 볼 수 없을 것 같습니다.) OverFitting 해결을 위한 Data enrichment (이미지 데이터에 다양한 변형을 가하거나 노이즈를 추가하는 등의 방법을 사용)를 사용 한 것도 좋은 퍼포먼스를 내는데 도움이 되었을 것 입니다. 정말 U-Net보다 결과가 좋네요!!! 모델 아키텍처 Encoding Path -&amp;gt; 640X640부터 40X40 까지 the features of interest를 검출 Decoding Path -&amp;gt; 40X40부터 640X640까지 synthesis를 예측 녹색 블록 -&amp;gt; regular convolutional layer, ReLu 활성함수, batch normalization으로 구성 보라색 블록 -&amp;gt; 3개의 convolutional block으로 구성되는데 마지막 블록3에는 블록1이 블록2와 skip해서 더해진 residual layer가 연결 됨 파란색 블록 -&amp;gt; maxpooling layer로써 encoding path에서 feature 압축을 위한 downsampling을 하기 위해 사용 빨간색 블록 -&amp;gt; deconvolutional layer로 decoding path에서 interpolation을 사용한 upsampling을 하기 위해 사용 참고자료 FusionNet : A deep fully residual convolutional neural network for image segmentation in connectomics 이상으로 FusionNet에 대한 글을 마칩니다. 수정해야 할 내용이 있다면 꼭 말씀 부탁드립니다. 부족한 글이지만 끝까지 읽어주셔서 감사합니다!!!</summary></entry><entry><title type="html">U-Net</title><link href="http://localhost:4000/U_Net" rel="alternate" type="text/html" title="U-Net" /><published>2018-04-02T09:00:00+09:00</published><updated>2018-04-02T09:00:00+09:00</updated><id>http://localhost:4000/U_Net</id><content type="html" xml:base="http://localhost:4000/U_Net">&lt;p&gt;논문 링크 : &lt;a href=&quot;https://arxiv.org/pdf/1505.04597.pdf&quot;&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이번 블로그의 내용은 Semantic Segmentation의 가장 기본적으로 많이 쓰이는 모델인 U-Net에 대한 내용입니다.&lt;/p&gt;

&lt;p&gt;U-Net의 이름은 그 자체로 모델의 형태가 U자로 되어 있어서 생긴 이름입니다.&lt;/p&gt;

&lt;p&gt;이번 블로그의 내용을 보시기 전에 앞전에 있는 &lt;a href=&quot;https://modulabs-biomedical.github.io/FCN&quot;&gt;Fully Convolution for Semantic Segmentation&lt;/a&gt; 과 
&lt;a href=&quot;https://modulabs-biomedical.github.io/Learning_Deconvolution_Network_for_Semantic_Segmentation&quot;&gt;Learning Deconvolution Network for Semantic Segmentation&lt;/a&gt; 을 읽고 보시면 더 도움이 되실 것 같습니다!&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Abstract에서는 전체적인 U-Net의 핵심 구조, data augmentation, ISBI challenge에서 좋은 성능을 보였다는 이야기를 하고 있었습니다.&lt;/p&gt;

&lt;p&gt;Deep Network는 많은 양의 annotated된 학습 샘플을 가지고 성공적인 training을 해왔습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 data augmentation을 잘 활용하여 annotated sample을 보다 효율적으로 사용하는 training 전략을 보여줍니다.&lt;/p&gt;

&lt;p&gt;논문에서 제안하는 아키텍쳐는 contracting path에서는 context를 캡쳐하고, 대칭적인 구조를 이루는 expanding path에서는 정교한 localization을 가능하게 하는 구조입니다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;논문에서 처음에 소개하는 내용은 지난 2년동안 (U-Net은 2015년 5월에 발표되었습니다.) deep convolution network는 많은 visual recognition 작업에서 매우 좋은 성능을 보였지만, training set의 크기와 고려할 네트워크의 크기 때문에 그 성공은 제한적이었다고 말하고 있습니다.&lt;/p&gt;

&lt;p&gt;Convolution네트워크의 일반적인 용도는 이미지에 대한 출력이 단일 클래스 레이블인 분류 작업에 있지만, 
많은 시각 작업, 특히 biomedical processing에서 원하는 출력은 localization을 포함해야 하며, 즉 클래스 라벨은 각 pixel에 할당 되어야 한다고 합니다.&lt;/p&gt;

&lt;p&gt;U-Net에서 핵심으로 말하고 있는 내용은 세가지로 생각됩니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Convolution Encoder에 해당하는 Contracting Path + Convolution Decoder에 해당하는 Expanding Path의 구조로 구성. (해당 구조는 Fully Convolution + Deconvolution 구조의 조합)&lt;/li&gt;
    &lt;li&gt;Expanding Path에서 Upsampling 할 때, 좀 더 정확한 Localization을 하기 위해서 Contracting Path의 Feature를 Copy and Crop하여 Concat 하는 구조.&lt;/li&gt;
    &lt;li&gt;Data Augmentation&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;기존에는(&lt;a href=&quot;http://people.idsia.ch/~juergen/nips2012.pdf&quot;&gt;Ciresan et al. [1]&lt;/a&gt;) Sliding-window을 하면서 로컬 영역(패치)을 입력으로 제공해서 각 픽셀의 클래스 레이블을 예측했지만, 이 방법은 2가지 단점으로 인해서 Fully Convolution Network구조를 제안하고 있습니다.&lt;/p&gt;

&lt;p&gt;두가지 단점은,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;네트워크가 각 패치에 대해 개별적으로 실행되어야 하고 패치가 겹쳐 중복성이 많기 때문에 상당히 느리다.&lt;/li&gt;
    &lt;li&gt;localization과 context사이에는 trade-off가 있는데, 이는 큰 사이즈의 patches는 많은 max-pooling을 해야해서 localization의 정확도가 떨어질 수 있고, 반면 작은 사이즈의 patches는 협소한 context만을 볼 수 있기 때문입니다.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Contracting Path에서 Pooling되기 전의 Feture들은 Upsampling 시에 Layer와 결합되어 고 해상도 output을 만들어 낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;하나 더 중요한 점은! 
많은 수의 Feature Channels를 사용하는데요.
아래 네트워크 아키텍쳐를 보시면 DownSampling시에는 64 채널 -&amp;gt; 1024채널까지 증가 되고,
UpSampling시에는 1024 채널 -&amp;gt; 64채널을 사용하고 있습니다.&lt;/p&gt;

&lt;p&gt;네트워크는 fully connected layers를 전혀 사용하지 않고, 각 layer에서 convolution만 사용합니다.&lt;/p&gt;

&lt;p&gt;다음으로, U-Net에서는 Segmentation시 overlab-tile 전략을 사용합니다. (그림.2)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-04-02-U_Net/u-net_fig_2.png&quot; alt=&quot;u-net_fig_2&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Overlap-tile 전략은, U-Net에서 다루는 전자 현미경 데이터의 특성상 이미지 사이즈의 크기가 상당히 크기 때문에 Patch 단위로 잘라서 Input 으로 넣고 있습니다.&lt;/p&gt;

  &lt;p&gt;이때 &lt;code class=&quot;highlighter-rouge&quot;&gt;Fig.2&lt;/code&gt;에서 보는 것과 같이 Border 부분에 정보가 없는 빈 부분을  0으로 채우거나, 주변의 값들로 채우거나 이런 방법이 아닌 Mirroring 방법으로 pixel의 값을 채워주는 방법 입니다.&lt;/p&gt;

  &lt;p&gt;노랑색 영역이 실제 세그멘테이션 될 영역이고, 파랑색 부분이 Patch 입니다.&lt;/p&gt;

  &lt;p&gt;그림을 확대해서 자세히 보시면, 거울처럼 반사되어 border부분이 채워진 것을 확인 할 수 있었습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-04-02-U_Net/u-net_fig_2_ex.png&quot; alt=&quot;u-net_fig_2_ex&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Overlap-tile 이라는 이름은, 파랑색 부분이 Patch단위로 잘라서 세그멘테이션을 하게 되는데 (용어는, Patch == Tile) 이 부분이 아래 그림처럼 겹쳐서 뜯어내서 학습시키기 때문인 것 같습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-04-02-U_Net/u-net_fig_2_overlap.png&quot; alt=&quot;u-net_fig_2_overlap&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-network-architecture&quot;&gt;2. Network Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-04-02-U_Net/u-net_fig_1.png&quot; alt=&quot;u-net_fig_1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Contracting Path는&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;전형적인 Convolution network 이고,&lt;/li&gt;
    &lt;li&gt;두번의 3X3 convolution을 반복 수행하며 (unpadded convolution를 사용),&lt;/li&gt;
    &lt;li&gt;ReLU를 사용합니다&lt;/li&gt;
    &lt;li&gt;2X2 max pooling 과 stride 2를 사용함&lt;/li&gt;
    &lt;li&gt;downsampling시에는 2배의 feture channel을 사용하고&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Expanding Path는&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;2X2 convolution (up-convolution)을 사용하고,&lt;/li&gt;
    &lt;li&gt;feature channel은 반으로 줄여 사용합니다.&lt;/li&gt;
    &lt;li&gt;Contracting Path에서 Max-Pooling 되기 전의 feature map을 Crop 하여 Up-Convolution 할 때 concatenation을 합니다.&lt;/li&gt;
    &lt;li&gt;두번의 3X3 convolution 반복하며&lt;/li&gt;
    &lt;li&gt;ReLU를 사용합니다&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;마지막 Final Layer에서는 1X1 convolution을 사용하여 2개의 클래스로 분류합니다.&lt;/p&gt;

&lt;p&gt;U-Net은 총 23개의 convolution layer가 사용됐습니다.&lt;/p&gt;

&lt;h2 id=&quot;3-training&quot;&gt;3. Training&lt;/h2&gt;

&lt;p&gt;학습은 Stochastic gradient descent 로 구현되었습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 학습시에 GPU memory의 사용량을 최대화 시키기 위해서 batch size를 크게해서 학습시키는 것 보다 input tile 의 size를 크게 주는 방법을 사용하는데요, 
이 방법으로 Batch Size가 작기 때문에, 이를 보완하고자 momentum의 값을 0.99값을 줘서 과거의 값들을 더 많이 반영하게 하여 학습이 더 잘 되도록 하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-04-02-U_Net/u-net_fig_3.png&quot; alt=&quot;u-net_fig_3&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;softmax&quot;&gt;softmax&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-04-02-U_Net/u-net_softmax.png&quot; alt=&quot;softmax&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;cross-entropy-loss-with-wx&quot;&gt;Cross Entropy Loss with w(x)&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;각각 정답 픽셀에대한 cross entropy loss에는 &lt;script type=&quot;math/tex&quot;&gt;w(\mathbf x)&lt;/script&gt;라는 가중치 값이 추가됩니다. 
여기서  &lt;script type=&quot;math/tex&quot;&gt;p_{l(x)}(x)&lt;/script&gt;의 &lt;script type=&quot;math/tex&quot;&gt;l(x)&lt;/script&gt;는 정답 클래스 즉 위 &lt;strong&gt;softmax 수식&lt;/strong&gt;에서 정답의 레이블에 해당하는  &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; 값을 반환하는 함수 입니다.&lt;/p&gt;

  &lt;p&gt;cross entropy 함수는 정답의 추정값을 log에 사용하기 때문에 이에 해당하는 정답의 확률을 가져오는 것이죠. 
수식 (1)은 loss 값에 가중치 &lt;script type=&quot;math/tex&quot;&gt;w(\mathbf x)&lt;/script&gt;를 곱한 형태이며 이제 우리가 살펴볼 것은  &lt;script type=&quot;math/tex&quot;&gt;w(\mathbf x)&lt;/script&gt;입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-04-02-U_Net/u-net_cross_entropy.png&quot; alt=&quot;1_cross_entropy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;w(\mathbf x)&lt;/script&gt; &lt;strong&gt;구하는 법 :&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-04-02-U_Net/u-net_wx.png&quot; alt=&quot;2_wx&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;w(\mathbf x)&lt;/script&gt;는 두개의 텀의 합으로 구성됩니다. &lt;script type=&quot;math/tex&quot;&gt;w(\mathbf x)&lt;/script&gt;는 &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; 위치의 픽셀에 가중치를 부여하는 함수입니다.&lt;/p&gt;

  &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;w_c(\mathbf x)&lt;/script&gt;는  &lt;script type=&quot;math/tex&quot;&gt;\mathbf x&lt;/script&gt; 의 위치에 해당하는 클래스의 빈도수에 따라 값이 결정됩니다.&lt;/p&gt;

  &lt;p&gt;즉 학습데이터에서 &lt;script type=&quot;math/tex&quot;&gt;\mathbf x&lt;/script&gt; 픽셀이 background일 경우가 많은지 foreground일 경우가 많은지의 빈도수에 따라 결정된다고 보시면 됩니다.&lt;/p&gt;

  &lt;p&gt;그 뒤의 exp 텀은 &lt;script type=&quot;math/tex&quot;&gt;d_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;d_2&lt;/script&gt; 함수를 포함하는데 &lt;script type=&quot;math/tex&quot;&gt;d_1&lt;/script&gt; 은 &lt;script type=&quot;math/tex&quot;&gt;\mathbf x&lt;/script&gt; 에서 가장 가까운 세포까지의 거리이고 &lt;script type=&quot;math/tex&quot;&gt;d_2&lt;/script&gt;는 두번째로 가까운 세포까지의 거리를 계산하는 함수입니다.&lt;/p&gt;

  &lt;p&gt;즉  &lt;script type=&quot;math/tex&quot;&gt;\mathbf x&lt;/script&gt;는 세포사이에 존재하는 픽셀이며 두 세포사이의 간격이 좁을 수록 weight를 큰 값으로 두 세포 사이가 넓을 수록 weight를 작은 값으로 갖게 됩니다. 이는 그림 3(d) 를 보시면 명확하게 확인하실 수 있습니다.&lt;/p&gt;

  &lt;p&gt;네트워크 파라미터의 초기화는 He 초기화 방법을 적용하였습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;31-data-augmentation&quot;&gt;3.1 Data Augmentation&lt;/h3&gt;

&lt;p&gt;Data Augmentation은 3 by 3 elastic 변환 행렬을 통해 수행합니다. &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformation_matrix&quot;&gt;영상변환 매트릭스&lt;/a&gt;는 클릭하셔서 위키의 내용을 참조하면 자세한 내용을 확인하실 수 있습니다. 세포를 세그멘테이션 하는 것이기 때문에 elastic deformation의 적용이 성능향상에 매우 큰 역할을 했다고 합니다.&lt;/p&gt;

&lt;h2 id=&quot;4-experiments&quot;&gt;4. Experiments&lt;/h2&gt;

&lt;p&gt;학습한 이후 성능 지표는 EM Segmentation challenge에서 wraping Error / Rand Error / Pixel Error로 1위를 한 지표를 볼 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-04-02-U_Net/u-net_table1.png&quot; alt=&quot;Table1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;IOU(Intersection over union) 방법으로 측정한 결과로는 아래와 같이 92% / 77.5%로 가장 좋은 성능을 보인것을 확인 할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-04-02-U_Net/u-net_table2.png&quot; alt=&quot;Table2&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h2&gt;

&lt;p&gt;마지막으로 결론입니다.&lt;/p&gt;

&lt;p&gt;U-Net 구조는 매우 다른 biomedical segmentation applications에서 좋은 성능을 보였고, 
이 성능을 보일 수 있었던 것은 Elastic 변환을 적용한 data augmentation 덕분이고, 
이것은 annotated image가 별로 없는 상황에서 매우 합리적이었다고 합니다.&lt;/p&gt;

&lt;p&gt;학습하는데 걸렸던 시간은 NVidia Titan GPU(6GB)를 사용했을때, 10시간이었습니다.&lt;/p&gt;

&lt;p&gt;이 U-Net 구현은 Caffe 기반으로 제공되고 있으며, U-Net의 아키텍쳐는 다양한 task에서 쉽게 적용되서 사용될 것을 확신한다고 하면서 논문은 마무리 됩니다.&lt;/p&gt;

&lt;p&gt;Image Segmentation Task에서 가장 많이 쓰이는 U-Net은 U자형 아케텍쳐와 Fully Convolution &amp;amp; Deconvolution 구조를 가지고 있는 것으로 좀 더 정확한 Localization을 위해서 Contracting Path의 Feature를 Copy and Crop해서 Expanding Path와 Concat하여 Upsampling을 한다는 것을 확실하게 알아두면 좋을 것 같습니다.&lt;/p&gt;

&lt;p&gt;이상 부족하지만, U-Net 논문에 대한 포스팅을 마치겠습니다. 틀린 부분이 있거나 보충 내용이 있으시면 언제든지 말씀해 주시면 수정하도록 하겠습니다!&lt;/p&gt;

&lt;p&gt;감사합니다.&lt;/p&gt;</content><author><name>강은숙</name></author><category term="논문 리뷰" /><summary type="html">논문 링크 : U-Net: Convolutional Networks for Biomedical Image Segmentation</summary></entry><entry><title type="html">Bias vs. Variance 개념 정리</title><link href="http://localhost:4000/Bias_vs_Variance" rel="alternate" type="text/html" title="Bias vs. Variance 개념 정리" /><published>2018-01-25T09:00:00+09:00</published><updated>2018-01-25T09:00:00+09:00</updated><id>http://localhost:4000/Bias_vs_Variance</id><content type="html" xml:base="http://localhost:4000/Bias_vs_Variance">&lt;p&gt;이 글에서 bias와 variance에 대해 살펴보려고 합니다. bias와 variance는 이미 많은 글이나 블로그에서 개념적으로 잘 설명되어 있습니다. 그럼에도 불구하고 다시 정리해보는 이유는 개념적으로 어느정도 이해는 되는데 좀 더 자세하게 보려고 하면, 블로그들의 예제들 간의 연결이 막혀서 헷갈리는 부분이 있어 이 글을 통해 확실히 이해하기 위해서 입니다.&lt;/p&gt;

&lt;h2 id=&quot;bias-vs-variance의-의미&quot;&gt;Bias vs. Variance의 의미&lt;/h2&gt;

&lt;p&gt;bias와 variance는 모델의 loss 또는 error를 의미합니다. 우리는 학습 모델의 bias, variance 특성을 구분하는 아래의 그림을 많이 보았습니다. 참고로 아래 그림은 bias-variance trade off를 나타내는 그림이 아닙니다. 아래 그림으로 trade off 관계를 설명하려고 하면 연결이 잘 안될 수 있습니다. (제가 이 글을 통해서 잘 연결 시켜보도록 하겠습니다.)&lt;/p&gt;

&lt;p&gt;bias-variance trade off는 잠 시 미뤄두고, bias와 variance의 의미를 파악하는데 집중해 보도록 하겠습니다. 아래 그림은 train data 또는 test data에 대한 결과를 bias와 variance 관점에서 해석하는 그림입니다. 붉은 색 영역은 target, 즉 참 값을 의미하고 파란 점은 추정 값을 의미합니다. 여기서 bias는 참 값들과 추정 값들의 차이(or 평균간의 거리)를 의미하고, variance는 추정 값들의 흩어진 정도를 의미합니다.(이 부분은 뒤에 Mean Square Error 수식을 통해 다시 설명하도록 하겠습니다.) bias와 variance가 loss이므로, 우리는 직관적으로  둘 다 작은 (a) 모델이 가장 좋은 모델인 것을 알 수 있습니다. (b)모델은 추정 값들을 평균한 값은 참 값과 비슷한데(bias가 작은데), 추정 값들의 variance가 커서 loss가 큰 모델입니다.  (c)는 bias가 크고, variance가 작은 모델이고, (d)는 둘 다 큰 모델입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-25-Bias_vs_Variance/3.jpg&quot; alt=&quot;high_low_bias_variance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림을 이제 train data와 test data 관점에서 살펴보도록 하겠습니다. train data에서는 (a)와 같은 결과 이었는데(train loss가 작았는데), test data를 넣어보니 (b),(c),(d)의 결과가 나왔다고 해 보겠습니다. 3 가지 모두 train data에서는 loss가 작았는데 test data에 적용해 보니 loss가 커졌습니다. 왜 그런 것일까요?&lt;/p&gt;

&lt;p&gt;(b),(c),(d)모두 에러가 크지만 서로 다른 유형의 에러를 나타내고 있습니다. 즉, 원인이 다르다는 것을 의미합니다. variance가 큰 (b)모델은 train data에 over-fitting된 것이 원인이고, 이는 너무 train data에 fitting된 모델을 만들어서 test data에서 오차가 발생한 것을 의미합니다. bias가 큰 (c) 모델은 &lt;u&gt;test data&lt;/u&gt;를 위한 학습이 덜 된 것이 원인이고, 이는 train data와 test data간의 차이가 너무 커서 train data로만 학습한 모델은 test data를 맞출수가 없는 것입니다. 만일, (c) 그림이 train data에 대한 것이라면 train data에 대해 under-fitting 즉, 학습이 덜 된 모델이라고 할 수 있습니다. (d)는 둘 다의 경우로 생각할 수 있겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;데이터 관점에서 보면 (b)의 경우 train data와 test data의 차이는 variance의 차이라고 할 수 있고,  (c)의 경우 train data와 test data의 차이는 평균의 차이라고도 할 수 있습니다&lt;/em&gt;. 학습 모델은 입력 X를 Y로 추정하는 것인데, 입력 X의 분포가 바뀌어 버리면 바뀐만큼의 error가 날 수 밖에 없습니다. 모델이 그렇게 설계되었기 때문입니다. train data와 test data의 차이가 많이 나면 어떻게 해야할까요? 즉, test data에 대해 (c)의 경우 어떻게해야 할까요? 힘들게 설계한 모델은 못쓰게 되는 것일까요? 우선적으로는 test data의 일부를 train set에 포함 시키는 것입니다. 그러나 이러한 방법은 test data에 대해서도 참 값을 labeling 해야하므로 비용도 많이 들고, 실 환경에서는 label이 없는 테스트 데이터가 들어 올 것이기 때문에 고려할 수 없게 됩니다. 이러한 문제를 해결하기 위한 분야가 바로 Domain Adaptation(DA)) 입니다. DA는 test data의 label(참값)없이 data set간의 차이를 줄여주는 네트워크를 기존 모델의 앞단에 추가하여 입력 데이터의 차이를 없애 주는 trick이라고 할 수 있겠습니다. 우리가 잘 학습한 기존 모델에 항상 유사한 입력을 넣어 오차를 줄여주겠다는 것입니다.(DA는 본 내용과 거리가 있으므로 개념적으로만 이해하면 좋을 것 같습니다.)&lt;/p&gt;

&lt;h2 id=&quot;bias-variance-trade-off&quot;&gt;Bias-Variance Trade Off&lt;/h2&gt;

&lt;p&gt;위의 train-test set의 관계가 bias-variance trade off로 내용으로 연결됩니다. train data에 너무 잘 맞게 학습시키는 것은 모델 복잡도를 높이는 것을 의미합니다. regression을 예로 생각해 보면 모든 데이터를 연결하는 선을 학습시켰다면 모델 복잡도는 매우 높게 되고 train loss는 ‘0’이 될 것입니다. 그러나, train data에 잘 맞게 만들기 위해 모델 복잡도를 너무 높이면 test data에는 그림1의 (b)처럼 variance가 커져서(bias는 작은데) total loss는 오히려 증가할 수 있습니다. 반대로, 모델 복잡도를 단순하게 가져가면 학습이 덜되서 그림 1의 (c)처럼 bias가 커서(variance는 작은데) total loss는 역시 증가할 수 있습니다. 여기서 말하는 bias와 variance는 서로 상반되어서 이를 &lt;strong&gt;bias-variance trade off&lt;/strong&gt; 라고 부릅니다.&lt;/p&gt;

&lt;p&gt;왜 그런 것일까요? 왜 모델 복잡도를 높이면 over-fit 될까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-25-Bias_vs_Variance/2.jpg&quot; alt=&quot;sampling_fluctuation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리가 sub set을 만들 때, 전체(통계에서 말하는 모집단)에서 샘플을 뽑아서 만들텐데, 뽑는 방법, 횟수 등에 따라 위 그림처럼 sub set 간의 변동이 발생하게 됩니다. 다시 말해서 train data와 test data는 이 변동 분 만큼 다를 수 있는 것입니다. train data는 이러한 변동을 포함하고 있는데, train error를 줄이기 위해 모델의 복잡도를 계속 높이면 이 변동 분까지 학습하게 됩니다. 아래 그림의 첫 번째가 이러한 내용을 설명한 것입니다. Linear-&amp;gt;Quadratic-&amp;gt;Spline으로 갈 수록 모델 복잡도가 올라가는 것을 의미하고, Quadratic이 detail한 부분까지 추정하는 것이 위에서의 언급된 샘플링 시의 변동 분까지 학습한 것이라고 할 수 있습니다. 아래의 두번 째 그림처럼 모델 복잡도를 계속 높이면 train loss는 계속해서 감소하지만, sub set간의 변동 분까지 학습하는 시점부터는 test error가 증가하게 됩니다. 이는 train data의 변동 분까지 학습하여 test data에 대한 추정 값의 variance가 증가한 그림1의 (b)와 같은 상황이라고 할 수 있습니다. 그래서 마지막 그림처럼 모델 복잡도가 올라갈 수록 bias는 감소하나 variance는 증가하는 bias-variance trade off 관계가 되는 것입니다. 마지막 그림에서 MSE는 test data의 total loss로 이해하시면 됩니다.&lt;/p&gt;

&lt;p&gt;참고: &lt;a href=&quot;https://gerardnico.com/wiki/data_mining/bias_trade-off&quot;&gt;https://gerardnico.com/wiki/data_mining/bias_trade-off&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-25-Bias_vs_Variance/4.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 최적의 모델 복잡도는 위 그림의 세번 째처럼 bias와 variance가 교차하는 부분에서 MSE or total test loss(bias와 variance의 합)가 가장 작은 점의 복잡도를 갖는 것입니다. 즉, 모델의 학습이 train error의 최소가 아닌 test error가 최소가 되도록 해야 한다는 것으로 이해할 수 있습니다. 추가적으로 고려해볼 방법은 처음부터 train set이 갖는 변동을 작게 만드는 것입니다. 이 방법이 바로 n-fold cross validation으로 여러 data set을 만들어 평균적으로 적용시킴으로써 sub set간의 변동을 줄이는 방법이라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;위 그림에서 total loss를 MSE로 표기했습니다. total loss는 bias와 variance loss를 모두 포함하고 있는 것으로 그렸는데 진짜 그럴까요?&lt;/p&gt;

&lt;p&gt;우리가 흔히 사용하는 MSE를 정리하여 다시 구성하면 아래와 같이 variance와 bias의 제곱 텀으로 표현될 수 있다.(sub-set의 변동 분은 제외한 수식입니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-25-Bias_vs_Variance/1.jpg&quot; alt=&quot;MSE&quot; /&gt;&lt;/p&gt;

&lt;p&gt;수식으로 부터 MSE의 variance와 bias의 의미를 다시 살펴보면,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;variance&lt;/strong&gt;는 &lt;u&gt;추정 값의 평균&lt;/u&gt;과 &lt;u&gt;추정 값들&lt;/u&gt;간의 차이에 대한 것이고, &lt;strong&gt;bias&lt;/strong&gt;는 &lt;u&gt;추정값의 평균&lt;/u&gt;과 &lt;u&gt;참 값들&lt;/u&gt;간의 차이에 대한 것입니다.  결국, variance는 추정 값들의 흩어진 정도이고, bias는 참 값과 추정 값의 거리를 의미합니다. 이 수식을 보면 &lt;strong&gt;&lt;u&gt;variance는 loss를 의미하지만, 참 값과는 관계없이 추정 값들의 흩어진 정도만을 의미&lt;/u&gt;&lt;/strong&gt;하고 있음을 유념해야 합니다.&lt;/p&gt;

&lt;p&gt;MSE가 bias와 variance로 구성되어 있고 bias-variance 사이에 trade off 관계인데, 우리가 학습하면 왜 MSE는 작아지고 ‘0’에 가까이 갈까요? 여기서는 trade off 관계가 없는 걸까요?&lt;/p&gt;

&lt;p&gt;위의 MSE의 수식을 보면 bias와 variance가 모두 제곱 텀이므로 둘 다 양수 입니다. 즉 MSE가 고정되면 하나가 커지면 하나는 작아지는 trade off 관계에 있습니다. 그런데 왜 학습을 하면 ‘0’에 가까이 수렴할까요?&lt;/p&gt;

&lt;p&gt;우리는 학습할 때 bias나 variance의 어느 한쪽을 보고 학습하는 것이 아니라 이 둘을 더한 MSE가 작아지도록 학습을 하기 때문에 최적 값을 알아서 찾아 학습을 하게 됩니다.(아마도 두 값이 거의 같을 때 최적 값이 될 것입니다.) 우리가 주로 고민해야하는 것은 train data들의 b-v trade off가 이니라 train 후 test 할 때의 b-v trade off라고 생각하시면 됩니다. 앞서 언급한 샘플링 시 발생하는 sub set의 변동 들에 대해 고려해야 한다는 것입니다.  train 에서 MSE의 최적 점을 찾았는데 train set과 test set의 차이로 test set에서는 이것이 최적점이 아닐 수 있다는 것입니다. (단, train error가 커서 train error에 대한 분석을 할 때는 train의 b-v에 대해서 고민이 필요할 것 같네요.)&lt;/p&gt;

&lt;p&gt;b-v trade off를 한번 더 반복해 보면, 모델 복잡도를 높이면 train data에 대한 bias와 variance는 계속해서 모두 감소합니다(bias+variance이 작아지도록 학습시키므로). 그런데 test data로 평가를 해보면 MSE loss가 어느시점부터 커지는데, 커지는 이유는 test data의 MSE loss 중에서 variance가 다시 커지기 때문이라고 이해하면 좋을 것 같습니다.&lt;/p&gt;

&lt;p&gt;그럼 딥러닝은 엄청나게 복잡도 높은 모델을 만드는데 왜 test data도 잘 맞출 수 있다고 할까요? over-fitting 문제가 없을까요?&lt;/p&gt;

&lt;p&gt;딥러닝도 똑같이 b-v trade off를 피할 수 없습니다. 딥러닝은 그래서 big 데이터가 전제가 되어야 합니다. big 데이터라는 것은 train data가 많다는 것을 의미하고, 이는 거의 전체 데이터(또는 모집단)와 비슷하다고 할수 있으며, 앞서 말한 sampling 변동이 거의 없다는 것을 전제로 하고 있습니다. 그렇기 때문에 모델 복잡도를 도 높일 수 있는 것입니다. 만일 train data가 적은 상태로 복잡한 딥러닝 모델을 적용하면 over-fitting 문제가 발생할 수 있습니다.  전제가 그렇다는 것이고 모든 경우에 big 데이터가 있는 것은 아니여서 딥러닝에서도 이러한 문제를 없애기 위한 다양한 trick(regularization, dropout, domain adaptation 등)이 존재합니다.&lt;/p&gt;

&lt;p&gt;결국, 딥러닝에서의 다양한 trick들은 loss를 줄이기 위한 것이고 , 이 loss는 bias와 variance로 이루어졌다는 것을 기억하면 좋을 것 같습니다. 그리고 그런한 trick들의 효과를 bias-variance 관점에서 생각해보면 다양한 딥러닝 trick들에 대한 더 좋은 이해가 되지 않을까 생각합니다.&lt;/p&gt;

&lt;p&gt;이해 안되는 부분이나 틀린 내용있으면 comment 부탁드립니다.&lt;/p&gt;

&lt;p&gt;다음에 시간이 되면 ensemble 기법인 bagging과 boosting을 사용했을 때 bias, variance가 어떻게 달라지는지 해석해볼 예정입니다.&lt;/p&gt;</content><author><name>홍규석</name></author><category term="개념 정리" /><summary type="html">이 글에서 bias와 variance에 대해 살펴보려고 합니다. bias와 variance는 이미 많은 글이나 블로그에서 개념적으로 잘 설명되어 있습니다. 그럼에도 불구하고 다시 정리해보는 이유는 개념적으로 어느정도 이해는 되는데 좀 더 자세하게 보려고 하면, 블로그들의 예제들 간의 연결이 막혀서 헷갈리는 부분이 있어 이 글을 통해 확실히 이해하기 위해서 입니다.</summary></entry><entry><title type="html">Learning Deconvolution Network for Semantic Segmentation</title><link href="http://localhost:4000/Learning_Deconvolution_Network_for_Semantic_Segmentation" rel="alternate" type="text/html" title="Learning Deconvolution Network for Semantic Segmentation" /><published>2018-01-03T09:00:00+09:00</published><updated>2018-01-03T09:00:00+09:00</updated><id>http://localhost:4000/Learning_Deconvolution_Network_for_Semantic_Segmentation</id><content type="html" xml:base="http://localhost:4000/Learning_Deconvolution_Network_for_Semantic_Segmentation">&lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1505.04366&quot;&gt;Noh, H., Hong, S., and Han, B. Learning Deconvolution Network for Semantic Segmentation. ICCV, 2015.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이번 논문은 &lt;a href=&quot;https://modulabs-biomedical.github.io/FCN&quot;&gt;앞서 다뤘던 Fully Convolutional Networks&lt;/a&gt;와 같은 년도(2015)에 다른 학회(FCN은 CVPR, 본 논문은 ICCV)에 발표된 논문입니다. FCN이나 이후에 다룰 UNet보다는 다소 인기가 적었지만, FCN이 가진 한계를 잘 짚어주셨다는 점에서 공부에 도움이 되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;fcn의-문제점&quot;&gt;FCN의 문제점&lt;/h2&gt;

&lt;h3 id=&quot;크기에-약하다&quot;&gt;크기에 약하다&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 예시들처럼 FCN의 추론 결과를 보면, 대상 물체가 너무 큰 경우(a)에는 파편화되고, 너무 작은 경우(b)에는 배경으로 무시되는 경향이 있습니다. FCN에서는 receptive field(상위 레이어의 한 지점에서 참조하는 하위 레이어의 영역)의 크기가 고정되어, 단일 배율(scale)만을 학습하는 것이 이 문제의 원인이라고 본 논문은 지적합니다. 여러 레이어의 결과를 조합하는 skip 구조가 이러한 현상을 완화시켜주기는 하지만, 근본적인 해법은 아니라는 주장입니다.&lt;/p&gt;

&lt;h3 id=&quot;디테일에-약하다&quot;&gt;디테일에 약하다&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig5.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FCN이 비록 기존 기법들에 비해 큰 발전을 이루었지만, 세부적인 영역을 찾아내는 데에서는 아직 개선의 여지가 있다고 이 논문은 보고 있습니다. FCN에서는 deconvolution에 들어가는 입력부터 이미 세부 묘사가 떨어지고, deconvolution 과정 자체도 충분히 깊지 않고 너무 단순하다고 말합니다.&lt;/p&gt;

&lt;h2 id=&quot;논문의-해법&quot;&gt;논문의 해법&lt;/h2&gt;

&lt;h3 id=&quot;네트워크-구조&quot;&gt;네트워크 구조&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;부족하면 더 넣으면 됩니다. FCN에서는 CNN의 결과를 입력 이미지의 원래 차원으로 확대(upsampling)하는 데에 deconvolution을 사용했지만, 이 논문에서는 deconvolution 시 차원을 유지하는 방법으로, CNN(논문에서 사용한 건 VGG-16)의 convolution만큼 레이어 숫자를 늘렸습니다. 결과적으로 거울에 비춘 모양이 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CNN으로 인해 원래 이미지보다 축소된 차원 크기는 unpooling으로 복원합니다. 여기서 unpooling이란 CNN의 max pooling 시의 위치 정보를 기억했다가, 원래 위치로 그대로 복원해주는 작업입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 효과는 위의 그림과 같습니다. (b)에서 (c)로 갈 때의 unpooling에 의해, 해상도가 커지는 대신 신호가 흩어져서 희소(sparse)해집니다. 이것을 (c)에서 (d)로 deconvolution을 거치면, 디테일을 살려내면서 신호가 고르게 밀집(dense)됩니다. 이러한 과정이 반복되자 노이즈도 점차 자연스럽게 사라지는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;학습-및-추론-방식&quot;&gt;학습 및 추론 방식&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/edge-box.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;단일 데이터셋에서 다양한 크기의 사례들을 학습하기 위해, 논문에서는 &lt;a href=&quot;&quot;&gt;edge-box&lt;/a&gt;라는 object proposal 알고리즘을 사용하여 무언가 있을만한 영역을 다양한 크기의 상자로 골라냅니다. 학습 시에는 우선 실제 정답이 가운데에 들어가도록 잘라낸(crop) 이미지들로 1차 학습을, 그 다음 edge-box의 결과물 중 실제 정답과 잘 겹치는 것들을 활용하여 조금 더 심도있는 2차 학습을 진행합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig6.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 학습에 사용된 edge-box는 추론 시에도 사용되는데, 추론 시 사용하는 object proposal의 수(상자 수)를 증가시킬 수록 성능은 좋아진다고 합니다. 물론 그만큼 계산량과 시간은 늘어납니다.&lt;/p&gt;

&lt;h2 id=&quot;결과&quot;&gt;결과&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig7.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 세심하게 설계되고 학습된 결과는 FCN이 실수하는 물체들도 보다 세밀하게 잘 찾아내는 모습을 보입니다. 다만 FCN이 잘 맞추는 곳에서 실수를 할 때도 있는데, 결국 둘을 앙상블하여 conditional random field로 후처리하면 두 가지 모델을 모두 뛰어넘게 되어, FCN과 상호 보완적인 관계에 있다고 논문은 맺습니다.&lt;/p&gt;

&lt;h2 id=&quot;추가-참고-문헌&quot;&gt;추가 참고 문헌&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.matthewzeiler.com/wp-content/uploads/2017/07/cvpr2010.pdf&quot;&gt;Zeiler, M. D. et al. Deconvolutional Networks. CVPR, 2010.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/edge-boxes-locating-object-proposals-from-edges/&quot;&gt;Zitnick, L. and Dollar, P. Edge Boxes: Locating Object Proposals from Edges. ECCV, 2014.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>오상준</name></author><category term="논문 리뷰" /><summary type="html">Noh, H., Hong, S., and Han, B. Learning Deconvolution Network for Semantic Segmentation. ICCV, 2015.</summary></entry><entry><title type="html">Fully Convolutional Networks for Semantic Segmentation</title><link href="http://localhost:4000/FCN" rel="alternate" type="text/html" title="Fully Convolutional Networks for Semantic Segmentation" /><published>2017-12-21T19:00:00+09:00</published><updated>2017-12-21T19:00:00+09:00</updated><id>http://localhost:4000/FCN</id><content type="html" xml:base="http://localhost:4000/FCN">&lt;p&gt;논문 링크 : &lt;a href=&quot;https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&quot;&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Semantic Segmentation는 영상을 pixel단위로 어떤 object인지 classification 하는 것이라고 볼 수 있습니다. (언제나 강력추천하는) &lt;a href=&quot;http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf&quot;&gt;cs231n 강의 자료&lt;/a&gt;를 보시면 쉽게 잘 나와 있죠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig1.jpg&quot; alt=&quot;&quot; /&gt;
Figure 1. Computer Vision Tasks&lt;/p&gt;

&lt;p&gt;예전에는 이렇게 pixel 단위로 classification을 하기 위해,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;그림 2와 같이 일정 영역을 포함하는 window를 만들고,&lt;/li&gt;
  &lt;li&gt;window 내 영상의 object를 classifiy해서&lt;/li&gt;
  &lt;li&gt;window 중앙의 pixel의 class 값이라고 간주하는&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;방식을 주로 사용했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig2.jpg&quot; alt=&quot;&quot; /&gt;
Figure 2.&lt;/p&gt;
&lt;strike&gt; 그림이 필요해서 커피숖에서 작업중에 급조를...&lt;/strike&gt;

&lt;p&gt;당연히 
&lt;strong&gt;계산량의 문제&lt;/strong&gt; 와 global information을 사용하지 못하고 window라는 제한된 영역의 &lt;strong&gt;local information만 사용한다&lt;/strong&gt; 는 문제가 있을 겁니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 
local feature와 global feature를 모두 사용하고
계산량 잡아먹는 주범인 fully connected layer를 없앤 CNN architecture를 제안합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h2&gt;
&lt;p&gt;Fully Convolutional Networks의 구조는 다음 그림 3과 같으며, 크게 4가지 부분으로 구성이 된다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig3.jpg&quot; alt=&quot;&quot; /&gt;
Figure 3. Architecture of Fully Convolutional Networks&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;네트워크 구조&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;(1) Feature Extraction&lt;/strong&gt; : 
 일반적인 CNN의 구조에서 많이 보이는 conv layer들로 구성되어 있습니다. 
 &lt;strong&gt;(2) Feature-level Classification&lt;/strong&gt; : 
 추출된 Feature map의 pixel 하나하나마다 classification을 수행합니다. 이 때 classification된 결과는 매우 coarse합니다. (그림 3에서 초록색 박스에 tabby cat class에 대한 Classification 결과 참고) 
 &lt;strong&gt;(3) Upsampling&lt;/strong&gt; :
 coarse 한 결과를 backward strided convolution 을 통해 upsampling하여 원래의 image size로 키워줍니다. 
&lt;strong&gt;(4) Segmentation&lt;/strong&gt; : 
각 class의 upsampling된 결과를 사용하여 하나의 Segmentation  결과 이미지를 만들어 줍니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그러면, 각 네트워크의 내부들을 한번 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;####&lt;em&gt;Feature Extraction&lt;/em&gt;
&lt;a href=&quot;https://arxiv.org/abs/1409.1556&quot;&gt;VGG-19 network&lt;/a&gt;를 feature extractor로 사용한다고 가정을 해봅시다. 이 경우 conv1 ~ conv5 layer ( or pool5 layer) 까지 통과하면서 feature를 추출합니다. 낮은 layer의 경우 작은 receptive field를 지니므로 작은 크기의 feature가, 높은 layer의 경우 높은 receptive field를 지니므로 큰 크기의 feature가 추출되게 되죠.
이렇게 추출된 최종 feature map (conv5 or pool5 layer)을 이용하여 다음 단계에서 coarse한 segmentation map 을 만들어냅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig4.jpg&quot; alt=&quot;&quot; /&gt;
Figure 4. Feature Extraction on VGG-19 Networks&lt;/p&gt;

&lt;h4 id=&quot;feature-level-classification&quot;&gt;&lt;em&gt;Feature-level Classification&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;원래 VGG network는 이렇게 추출된 feature의 뒤에 4096, 4096, 1000으로 이어지는 fully connected layer를 연결하여 classification을 합니다만(그림 4), 본 논문에서는 이런 fully connected layer를 없애버립니다. 그리고, 1x1 conv layer를 추가합니다. 그림 3에 보시면 4096, 4096, 21 이라고 표현된 1x1 conv layer를 보실 수 있는데요. 
(1000이 21로 바뀐 이유는 이 논문에서는 PASCAL VOC dataset으로 실험을 하는데, 그 데이터의 클래스가 20개 + background 이기 때문입니다.)
이 1x1 conv 의 결과물이 결국 각 class의 feature map 상에서의 classifiation (즉, segmentation) 이 됩니다. 그림 3에 보면 conv8(마지막 1x1 conv) layer의 depth channel 중에서 tabby cat에 해당하는 class의 feature map  상에서의 classification (즉, segmentation) 결과 heatmap 을 볼 수 있습니다. 마지막 1x1 conv layer에서 depth channel은 각 class를 의미하므로, 어떤 class의 segmentation heatmap도 추정할 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;upsampling&quot;&gt;&lt;em&gt;Upsampling&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;그런데 feature map level에서 segmentation 한 결과는 너무 coarse한 결과입니다. (그림 3의 tabby cat heatmap 보면 깍두기처럼…) 따라서, 이 coarse한 heatmap을 dense하게 (원래의 image size로) 만들어주어야 합니다. 본 논문에서는 upsampling(backwards strided convolution)을 사용합니다. 
그러면 각 class별로 dense한 segmentation 결과를 얻을 수 있습니다. 즉, 원래 image의 폭을 W, 높이를 H, 라고 한다면, WxHx21 의 dense heatmap결과를 얻을 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;segmentation&quot;&gt;&lt;em&gt;Segmentation&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;그러나 우리는 결국 각 class 별 결과를 추정하고자 하는 것이 아니죠. 하나의 이미지에서 모든 class의 segmentation된 결과를 얻어야 합니다. 그래서 윗 단계에서 얻어진 upsampling된 각 class별 heatmap을 softmax를 이용하여 가장 높은 확률을 가지는 class만 모아서 한장의 segmentation 이미지로 만듭니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;skip-combining&quot;&gt;Skip Combining&lt;/h2&gt;
&lt;p&gt;그런데 3단계의 &lt;strong&gt;&lt;em&gt;Upsampling&lt;/em&gt;&lt;/strong&gt; 과정에서 coarse한 결과를 dense하게 만들어줄 때 너무 많이 뻥튀기를 하기 때문에 detail아 다 뭉개진 segmenation 결과를 얻을 수 밖에 없습니다. (그림 5 참고)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig5.jpg&quot; alt=&quot;&quot; /&gt;
Figure 5. Segmentation Result from the Last Conv Layer&lt;/p&gt;

&lt;p&gt;다음 그림 6과 같은 CNN 구조의 Fully Convolutional Networks가 있다고 가정해봅시다. 최종 결과물인 FCN-32s는 32배로 upsampling을 하기 때문에 detail이 많이 사라진 segmentation 결과를 보여줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig6.jpg&quot; alt=&quot;&quot; /&gt;
Figure 6. (Conventional) Fully Convolutional Networks : FCN-32s&lt;/p&gt;

&lt;p&gt;본 논문에서는 이 문제를 해결하기 위해 그 이전 layer의 feature map을 이용하는 skip combining 기법을 사용합니다(그림 7 참고).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig7.jpg&quot; alt=&quot;&quot; /&gt;
Figure 7. Fully Convolutional Networks with Skip Combining : FCN-16s&lt;/p&gt;

&lt;p&gt;그림 6에서는 마지막 conv layer인 conv 7에서 32배 upsampling하여 segmentation 결과를 만들었습니다. 하지만, 그림 7에서는 마지막 conv layer 결과를 2배 upsampling 하고 마지막 pooling layer (pool5)이전 단계 즉, pool 4 layer의 결과와 합쳐줍니다. 그리고 난 후, 그 합쳐진 결과를 16배 upsampling 하여 FCN-16s 라는 segmentation 결과 이미지를 만들어 냅니다.&lt;/p&gt;

&lt;p&gt;여기서 합친다는 말은 그냥 &lt;strong&gt;더해준다&lt;/strong&gt;는 의미입니다. 
&lt;a href=&quot;https://github.com/shekkizh/FCN.tensorflow/blob/master/FCN.py&quot;&gt;다음 코드&lt;/a&gt;를 참고하세요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig7.code.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 하나 더 이전의 pooling layer와도 합칠 수 있지 않을까요? 당연히 있습니다.
그림 8은 pool 3 layer + 2배 upsampling된 pool 4 layer + 4배 upsampling 된 conv7 layer 값을 다시 8배 upsampling 하여 FCN-8s라는 보다 detail한 segmentation 이미지를 만들어내는 구조입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig8.jpg&quot; alt=&quot;&quot; /&gt;
Figure 8. Fully Convolutional Networks with Skip Combining : FCN-8s&lt;/p&gt;

&lt;p&gt;자 그러면 각 skip combining 한 후의 최종 segmentation 결과를 살펴볼까요? 확실히 FCN-32s에 비해 FCN-16s가, FCN-16s에 비해 FCN-8s가 detail한 segmentation 결과를 보여줌을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig9.jpg&quot; alt=&quot;&quot; /&gt;
Figure 9. Segmentation Results&lt;/p&gt;</content><author><name>김승일</name></author><category term="논문 리뷰" /><summary type="html">논문 링크 : Fully Convolutional Networks for Semantic Segmentation</summary></entry></feed>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2018-01-17T21:36:27+09:00</updated><id>/</id><title type="html">바이오메디컬랩@모두의연구소</title><subtitle>모두의연구소 바이오메디컬랩 기술 블로그입니다.</subtitle><entry><title type="html">Learning Deconvolution Network for Semantic Segmentation</title><link href="/Learning_Deconvolution_Network_for_Semantic_Segmentation" rel="alternate" type="text/html" title="Learning Deconvolution Network for Semantic Segmentation" /><published>2018-01-03T09:00:00+09:00</published><updated>2018-01-03T09:00:00+09:00</updated><id>/Learning_Deconvolution_Network_for_Semantic_Segmentation</id><content type="html" xml:base="/Learning_Deconvolution_Network_for_Semantic_Segmentation">&lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1505.04366&quot;&gt;Noh, H., Hong, S., and Han, B. Learning Deconvolution Network for Semantic Segmentation. ICCV, 2015.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이번 논문은 &lt;a href=&quot;https://modulabs-biomedical.github.io/FCN&quot;&gt;앞서 다뤘던 Fully Convolutional Networks&lt;/a&gt;와 같은 년도(2015)에 다른 학회(FCN은 CVPR, 본 논문은 ICCV)에 발표된 논문입니다. FCN이나 이후에 다룰 UNet보다는 다소 인기가 적었지만, FCN이 가진 한계를 잘 짚어주셨다는 점에서 공부에 도움이 되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;fcn의-문제점&quot;&gt;FCN의 문제점&lt;/h2&gt;

&lt;h3 id=&quot;크기에-약하다&quot;&gt;크기에 약하다&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 예시들처럼 FCN의 추론 결과를 보면, 대상 물체가 너무 큰 경우(a)에는 파편화되고, 너무 작은 경우(b)에는 배경으로 무시되는 경향이 있습니다. FCN에서는 receptive field(상위 레이어의 한 지점에서 참조하는 하위 레이어의 영역)의 크기가 고정되어, 단일 배율(scale)만을 학습하는 것이 이 문제의 원인이라고 본 논문은 지적합니다. 여러 레이어의 결과를 조합하는 skip 구조가 이러한 현상을 완화시켜주기는 하지만, 근본적인 해법은 아니라는 주장입니다.&lt;/p&gt;

&lt;h3 id=&quot;디테일에-약하다&quot;&gt;디테일에 약하다&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig5.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FCN이 비록 기존 기법들에 비해 큰 발전을 이루었지만, 세부적인 영역을 찾아내는 데에서는 아직 개선의 여지가 있다고 이 논문은 보고 있습니다. FCN에서는 deconvolution에 들어가는 입력부터 이미 세부 묘사가 떨어지고, deconvolution 과정 자체도 충분히 깊지 않고 너무 단순하다고 말합니다.&lt;/p&gt;

&lt;h2 id=&quot;논문의-해법&quot;&gt;논문의 해법&lt;/h2&gt;

&lt;h3 id=&quot;네트워크-구조&quot;&gt;네트워크 구조&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;부족하면 더 넣으면 됩니다. FCN에서는 CNN의 결과를 입력 이미지의 원래 차원으로 확대(upsampling)하는 데에 deconvolution을 사용했지만, 이 논문에서는 deconvolution 시 차원을 유지하는 방법으로, CNN(논문에서 사용한 건 VGG-16)의 convolution만큼 레이어 숫자를 늘렸습니다. 결과적으로 거울에 비춘 모양이 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CNN으로 인해 원래 이미지보다 축소된 차원 크기는 unpooling으로 복원합니다. 여기서 unpooling이란 CNN의 max pooling 시의 위치 정보를 기억했다가, 원래 위치로 그대로 복원해주는 작업입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 효과는 위의 그림과 같습니다. (b)에서 (c)로 갈 때의 unpooling에 의해, 해상도가 커지는 대신 신호가 흩어져서 희소(sparse)해집니다. 이것을 (c)에서 (d)로 deconvolution을 거치면, 디테일을 살려내면서 신호가 고르게 밀집(dense)됩니다. 이러한 과정이 반복되자 노이즈도 점차 자연스럽게 사라지는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;학습-및-추론-방식&quot;&gt;학습 및 추론 방식&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/edge-box.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;단일 데이터셋에서 다양한 크기의 사례들을 학습하기 위해, 논문에서는 &lt;a href=&quot;&quot;&gt;edge-box&lt;/a&gt;라는 object proposal 알고리즘을 사용하여 무언가 있을만한 영역을 다양한 크기의 상자로 골라냅니다. 학습 시에는 우선 실제 정답이 가운데에 들어가도록 잘라낸(crop) 이미지들로 1차 학습을, 그 다음 edge-box의 결과물 중 실제 정답과 잘 겹치는 것들을 활용하여 조금 더 심도있는 2차 학습을 진행합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig6.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 학습에 사용된 edge-box는 추론 시에도 사용되는데, 추론 시 사용하는 object proposal의 수(상자 수)를 증가시킬 수록 성능은 좋아진다고 합니다. 물론 그만큼 계산량과 시간은 늘어납니다.&lt;/p&gt;

&lt;h2 id=&quot;결과&quot;&gt;결과&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig7.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 세심하게 설계되고 학습된 결과는 FCN이 실수하는 물체들도 보다 세밀하게 잘 찾아내는 모습을 보입니다. 다만 FCN이 잘 맞추는 곳에서 실수를 할 때도 있는데, 결국 둘을 앙상블하여 conditional random field로 후처리하면 두 가지 모델을 모두 뛰어넘게 되어, FCN과 상호 보완적인 관계에 있다고 논문은 맺습니다.&lt;/p&gt;

&lt;h2 id=&quot;추가-참고-문헌&quot;&gt;추가 참고 문헌&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.matthewzeiler.com/wp-content/uploads/2017/07/cvpr2010.pdf&quot;&gt;Zeiler, M. D. et al. Deconvolutional Networks. CVPR, 2010.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/edge-boxes-locating-object-proposals-from-edges/&quot;&gt;Zitnick, L. and Dollar, P. Edge Boxes: Locating Object Proposals from Edges. ECCV, 2014.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>오상준</name></author><category term="논문 리뷰" /><summary type="html">Noh, H., Hong, S., and Han, B. Learning Deconvolution Network for Semantic Segmentation. ICCV, 2015.</summary></entry><entry><title type="html">Fully Convolutional Networks for Semantic Segmentation</title><link href="/FCN" rel="alternate" type="text/html" title="Fully Convolutional Networks for Semantic Segmentation" /><published>2017-12-21T19:00:00+09:00</published><updated>2017-12-21T19:00:00+09:00</updated><id>/FCN</id><content type="html" xml:base="/FCN">&lt;p&gt;논문 링크 : &lt;a href=&quot;https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&quot;&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Semantic Segmentation는 영상을 pixel단위로 어떤 object인지 classification 하는 것이라고 볼 수 있습니다. (언제나 강력추천하는) &lt;a href=&quot;http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf&quot;&gt;cs231n 강의 자료&lt;/a&gt;를 보시면 쉽게 잘 나와 있죠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig1.jpg&quot; alt=&quot;&quot; /&gt;
Figure 1. Computer Vision Tasks&lt;/p&gt;

&lt;p&gt;예전에는 이렇게 pixel 단위로 classification을 하기 위해,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;그림 2와 같이 일정 영역을 포함하는 window를 만들고,&lt;/li&gt;
  &lt;li&gt;window 내 영상의 object를 classifiy해서&lt;/li&gt;
  &lt;li&gt;window 중앙의 pixel의 class 값이라고 간주하는&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;방식을 주로 사용했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig2.jpg&quot; alt=&quot;&quot; /&gt;
Figure 2.&lt;/p&gt;
&lt;strike&gt; 그림이 필요해서 커피숖에서 작업중에 급조를...&lt;/strike&gt;

&lt;p&gt;당연히 
&lt;strong&gt;계산량의 문제&lt;/strong&gt; 와 global information을 사용하지 못하고 window라는 제한된 영역의 &lt;strong&gt;local information만 사용한다&lt;/strong&gt; 는 문제가 있을 겁니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 
local feature와 global feature를 모두 사용하고
계산량 잡아먹는 주범인 fully connected layer를 없앤 CNN architecture를 제안합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h2&gt;
&lt;p&gt;Fully Convolutional Networks의 구조는 다음 그림 3과 같으며, 크게 4가지 부분으로 구성이 된다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig3.jpg&quot; alt=&quot;&quot; /&gt;
Figure 3. Architecture of Fully Convolutional Networks&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;네트워크 구조&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;(1) Feature Extraction&lt;/strong&gt; : 
 일반적인 CNN의 구조에서 많이 보이는 conv layer들로 구성되어 있습니다. 
 &lt;strong&gt;(2) Feature-level Classification&lt;/strong&gt; : 
 추출된 Feature map의 pixel 하나하나마다 classification을 수행합니다. 이 때 classification된 결과는 매우 coarse합니다. (그림 3에서 초록색 박스에 tabby cat class에 대한 Classification 결과 참고) 
 &lt;strong&gt;(3) Upsampling&lt;/strong&gt; :
 coarse 한 결과를 backward strided convolution 을 통해 upsampling하여 원래의 image size로 키워줍니다. 
&lt;strong&gt;(4) Segmentation&lt;/strong&gt; : 
각 class의 upsampling된 결과를 사용하여 하나의 Segmentation  결과 이미지를 만들어 줍니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그러면, 각 네트워크의 내부들을 한번 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;####&lt;em&gt;Feature Extraction&lt;/em&gt;
&lt;a href=&quot;https://arxiv.org/abs/1409.1556&quot;&gt;VGG-19 network&lt;/a&gt;를 feature extractor로 사용한다고 가정을 해봅시다. 이 경우 conv1 ~ conv5 layer ( or pool5 layer) 까지 통과하면서 feature를 추출합니다. 낮은 layer의 경우 작은 receptive field를 지니므로 작은 크기의 feature가, 높은 layer의 경우 높은 receptive field를 지니므로 큰 크기의 feature가 추출되게 되죠.
이렇게 추출된 최종 feature map (conv5 or pool5 layer)을 이용하여 다음 단계에서 coarse한 segmentation map 을 만들어냅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig4.jpg&quot; alt=&quot;&quot; /&gt;
Figure 4. Feature Extraction on VGG-19 Networks&lt;/p&gt;

&lt;h4 id=&quot;feature-level-classification&quot;&gt;&lt;em&gt;Feature-level Classification&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;원래 VGG network는 이렇게 추출된 feature의 뒤에 4096, 4096, 1000으로 이어지는 fully connected layer를 연결하여 classification을 합니다만(그림 4), 본 논문에서는 이런 fully connected layer를 없애버립니다. 그리고, 1x1 conv layer를 추가합니다. 그림 3에 보시면 4096, 4096, 21 이라고 표현된 1x1 conv layer를 보실 수 있는데요. 
(1000이 21로 바뀐 이유는 이 논문에서는 PASCAL VOC dataset으로 실험을 하는데, 그 데이터의 클래스가 20개 + background 이기 때문입니다.)
이 1x1 conv 의 결과물이 결국 각 class의 feature map 상에서의 classifiation (즉, segmentation) 이 됩니다. 그림 3에 보면 conv8(마지막 1x1 conv) layer의 depth channel 중에서 tabby cat에 해당하는 class의 feature map  상에서의 classification (즉, segmentation) 결과 heatmap 을 볼 수 있습니다. 마지막 1x1 conv layer에서 depth channel은 각 class를 의미하므로, 어떤 class의 segmentation heatmap도 추정할 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;upsampling&quot;&gt;&lt;em&gt;Upsampling&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;그런데 feature map level에서 segmentation 한 결과는 너무 coarse한 결과입니다. (그림 3의 tabby cat heatmap 보면 깍두기처럼…) 따라서, 이 coarse한 heatmap을 dense하게 (원래의 image size로) 만들어주어야 합니다. 본 논문에서는 upsampling(backwards strided convolution)을 사용합니다. 
그러면 각 class별로 dense한 segmentation 결과를 얻을 수 있습니다. 즉, 원래 image의 폭을 W, 높이를 H, 라고 한다면, WxHx21 의 dense heatmap결과를 얻을 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;segmentation&quot;&gt;&lt;em&gt;Segmentation&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;그러나 우리는 결국 각 class 별 결과를 추정하고자 하는 것이 아니죠. 하나의 이미지에서 모든 class의 segmentation된 결과를 얻어야 합니다. 그래서 윗 단계에서 얻어진 upsampling된 각 class별 heatmap을 softmax를 이용하여 가장 높은 확률을 가지는 class만 모아서 한장의 segmentation 이미지로 만듭니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;skip-combining&quot;&gt;Skip Combining&lt;/h2&gt;
&lt;p&gt;그런데 3단계의 &lt;strong&gt;&lt;em&gt;Upsampling&lt;/em&gt;&lt;/strong&gt; 과정에서 coarse한 결과를 dense하게 만들어줄 때 너무 많이 뻥튀기를 하기 때문에 detail아 다 뭉개진 segmenation 결과를 얻을 수 밖에 없습니다. (그림 5 참고)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig5.jpg&quot; alt=&quot;&quot; /&gt;
Figure 5. Segmentation Result from the Last Conv Layer&lt;/p&gt;

&lt;p&gt;다음 그림 6과 같은 CNN 구조의 Fully Convolutional Networks가 있다고 가정해봅시다. 최종 결과물인 FCN-32s는 32배로 upsampling을 하기 때문에 detail이 많이 사라진 segmentation 결과를 보여줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig6.jpg&quot; alt=&quot;&quot; /&gt;
Figure 6. (Conventional) Fully Convolutional Networks : FCN-32s&lt;/p&gt;

&lt;p&gt;본 논문에서는 이 문제를 해결하기 위해 그 이전 layer의 feature map을 이용하는 skip combining 기법을 사용합니다(그림 7 참고).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig7.jpg&quot; alt=&quot;&quot; /&gt;
Figure 7. Fully Convolutional Networks with Skip Combining : FCN-16s&lt;/p&gt;

&lt;p&gt;그림 6에서는 마지막 conv layer인 conv 7에서 32배 upsampling하여 segmentation 결과를 만들었습니다. 하지만, 그림 7에서는 마지막 conv layer 결과를 2배 upsampling 하고 마지막 pooling layer (pool5)이전 단계 즉, pool 4 layer의 결과와 합쳐줍니다. 그리고 난 후, 그 합쳐진 결과를 16배 upsampling 하여 FCN-16s 라는 segmentation 결과 이미지를 만들어 냅니다.&lt;/p&gt;

&lt;p&gt;여기서 합친다는 말은 그냥 &lt;strong&gt;더해준다&lt;/strong&gt;는 의미입니다. 
&lt;a href=&quot;https://github.com/shekkizh/FCN.tensorflow/blob/master/FCN.py&quot;&gt;다음 코드&lt;/a&gt;를 참고하세요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig7.code.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 하나 더 이전의 pooling layer와도 합칠 수 있지 않을까요? 당연히 있습니다.
그림 8은 pool 3 layer + 2배 upsampling된 pool 4 layer + 4배 upsampling 된 conv7 layer 값을 다시 8배 upsampling 하여 FCN-8s라는 보다 detail한 segmentation 이미지를 만들어내는 구조입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig8.jpg&quot; alt=&quot;&quot; /&gt;
Figure 8. Fully Convolutional Networks with Skip Combining : FCN-8s&lt;/p&gt;

&lt;p&gt;자 그러면 각 skip combining 한 후의 최종 segmentation 결과를 살펴볼까요? 확실히 FCN-32s에 비해 FCN-16s가, FCN-16s에 비해 FCN-8s가 detail한 segmentation 결과를 보여줌을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig9.jpg&quot; alt=&quot;&quot; /&gt;
Figure 9. Segmentation Results&lt;/p&gt;</content><author><name>김승일</name></author><category term="논문 리뷰" /><summary type="html">논문 링크 : Fully Convolutional Networks for Semantic Segmentation</summary></entry></feed>
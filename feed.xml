<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2018-02-18T12:34:06+09:00</updated><id>/</id><title type="html">바이오메디컬랩@모두의연구소</title><subtitle>모두의연구소 바이오메디컬랩 기술 블로그입니다.</subtitle><entry><title type="html">Bias vs. Variance 개념 정리</title><link href="/Bias_vs_Variance" rel="alternate" type="text/html" title="Bias vs. Variance 개념 정리" /><published>2018-01-25T09:00:00+09:00</published><updated>2018-01-25T09:00:00+09:00</updated><id>/Bias_vs_Variance</id><content type="html" xml:base="/Bias_vs_Variance">&lt;p&gt;이 글에서 bias와 variance에 대해 살펴보려고 합니다. bias와 variance는 이미 많은 글이나 블로그에서 개념적으로 잘 설명되어 있습니다. 그럼에도 불구하고 다시 정리해보는 이유는 개념적으로 어느정도 이해는 되는데 좀 더 자세하게 보려고 하면, 블로그들의 예제들 간의 연결이 막혀서 헷갈리는 부분이 있어 이 글을 통해 확실히 이해하기 위해서 입니다.&lt;/p&gt;

&lt;h2 id=&quot;bias-vs-variance의-의미&quot;&gt;Bias vs. Variance의 의미&lt;/h2&gt;

&lt;p&gt;bias와 variance는 모델의 loss 또는 error를 의미합니다. 우리는 학습 모델의 bias, variance 특성을 구분하는 아래의 그림을 많이 보았습니다. 참고로 아래 그림은 bias-variance trade off를 나타내는 그림이 아닙니다. 아래 그림으로 trade off 관계를 설명하려고 하면 연결이 잘 안될 수 있습니다. (제가 이 글을 통해서 잘 연결 시켜보도록 하겠습니다.)&lt;/p&gt;

&lt;p&gt;bias-variance trade off는 잠 시 미뤄두고, bias와 variance의 의미를 파악하는데 집중해 보도록 하겠습니다. 아래 그림은 train data 또는 test data에 대한 결과를 bias와 variance 관점에서 해석하는 그림입니다. 붉은 색 영역은 target, 즉 참 값을 의미하고 파란 점은 추정 값을 의미합니다. 여기서 bias는 참 값들과 추정 값들의 차이(or 평균간의 거리)를 의미하고, variance는 추정 값들의 흩어진 정도를 의미합니다.(이 부분은 뒤에 Mean Square Error 수식을 통해 다시 설명하도록 하겠습니다.) bias와 variance가 loss이므로, 우리는 직관적으로  둘 다 작은 (a) 모델이 가장 좋은 모델인 것을 알 수 있습니다. (b)모델은 추정 값들을 평균한 값은 참 값과 비슷한데(bias가 작은데), 추정 값들의 variance가 커서 loss가 큰 모델입니다.  (c)는 bias가 크고, variance가 작은 모델이고, (d)는 둘 다 큰 모델입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-25-Bias_vs_Variance/3.jpg&quot; alt=&quot;high_low_bias_variance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림을 이제 train data와 test data 관점에서 살펴보도록 하겠습니다. train data에서는 (a)와 같은 결과 이었는데(train loss가 작았는데), test data를 넣어보니 (b),(c),(d)의 결과가 나왔다고 해 보겠습니다. 3 가지 모두 train data에서는 loss가 작았는데 test data에 적용해 보니 loss가 커졌습니다. 왜 그런 것일까요?&lt;/p&gt;

&lt;p&gt;(b),(c),(d)모두 에러가 크지만 서로 다른 유형의 에러를 나타내고 있습니다. 즉, 원인이 다르다는 것을 의미합니다. variance가 큰 (b)모델은 train data에 over-fitting된 것이 원인이고, 이는 너무 train data에 fitting된 모델을 만들어서 test data에서 오차가 발생한 것을 의미합니다. bias가 큰 (c) 모델은 &lt;u&gt;test data&lt;/u&gt;를 위한 학습이 덜 된 것이 원인이고, 이는 train data와 test data간의 차이가 너무 커서 train data로만 학습한 모델은 test data를 맞출수가 없는 것입니다. 만일, (c) 그림이 train data에 대한 것이라면 train data에 대해 under-fitting 즉, 학습이 덜 된 모델이라고 할 수 있습니다. (d)는 둘 다의 경우로 생각할 수 있겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;데이터 관점에서 보면 (b)의 경우 train data와 test data의 차이는 variance의 차이라고 할 수 있고,  (c)의 경우 train data와 test data의 차이는 평균의 차이라고도 할 수 있습니다&lt;/em&gt;. 학습 모델은 입력 X를 Y로 추정하는 것인데, 입력 X의 분포가 바뀌어 버리면 바뀐만큼의 error가 날 수 밖에 없습니다. 모델이 그렇게 설계되었기 때문입니다. train data와 test data의 차이가 많이 나면 어떻게 해야할까요? 즉, test data에 대해 (c)의 경우 어떻게해야 할까요? 힘들게 설계한 모델은 못쓰게 되는 것일까요? 우선적으로는 test data의 일부를 train set에 포함 시키는 것입니다. 그러나 이러한 방법은 test data에 대해서도 참 값을 labeling 해야하므로 비용도 많이 들고, 실 환경에서는 label이 없는 테스트 데이터가 들어 올 것이기 때문에 고려할 수 없게 됩니다. 이러한 문제를 해결하기 위한 분야가 바로 Domain Adaptation(DA)) 입니다. DA는 test data의 label(참값)없이 data set간의 차이를 줄여주는 네트워크를 기존 모델의 앞단에 추가하여 입력 데이터의 차이를 없애 주는 trick이라고 할 수 있겠습니다. 우리가 잘 학습한 기존 모델에 항상 유사한 입력을 넣어 오차를 줄여주겠다는 것입니다.(DA는 본 내용과 거리가 있으므로 개념적으로만 이해하면 좋을 것 같습니다.)&lt;/p&gt;

&lt;h2 id=&quot;bias-variance-trade-off&quot;&gt;Bias-Variance Trade Off&lt;/h2&gt;

&lt;p&gt;위의 train-test set의 관계가 bias-variance trade off로 내용으로 연결됩니다. train data에 너무 잘 맞게 학습시키는 것은 모델 복잡도를 높이는 것을 의미합니다. regression을 예로 생각해 보면 모든 데이터를 연결하는 선을 학습시켰다면 모델 복잡도는 매우 높게 되고 train loss는 ‘0’이 될 것입니다. 그러나, train data에 잘 맞게 만들기 위해 모델 복잡도를 너무 높이면 test data에는 그림1의 (b)처럼 variance가 커져서(bias는 작은데) total loss는 오히려 증가할 수 있습니다. 반대로, 모델 복잡도를 단순하게 가져가면 학습이 덜되서 그림 1의 (c)처럼 bias가 커서(variance는 작은데) total loss는 역시 증가할 수 있습니다. 여기서 말하는 bias와 variance는 서로 상반되어서 이를 &lt;strong&gt;bias-variance trade off&lt;/strong&gt; 라고 부릅니다.&lt;/p&gt;

&lt;p&gt;왜 그런 것일까요? 왜 모델 복잡도를 높이면 over-fit 될까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-25-Bias_vs_Variance/2.jpg&quot; alt=&quot;sampling_fluctuation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리가 sub set을 만들 때, 전체(통계에서 말하는 모집단)에서 샘플을 뽑아서 만들텐데, 뽑는 방법, 횟수 등에 따라 위 그림처럼 sub set 간의 변동이 발생하게 됩니다. 다시 말해서 train data와 test data는 이 변동 분 만큼 다를 수 있는 것입니다. train data는 이러한 변동을 포함하고 있는데, train error를 줄이기 위해 모델의 복잡도를 계속 높이면 이 변동 분까지 학습하게 됩니다. 아래 그림의 첫 번째가 이러한 내용을 설명한 것입니다. Linear-&amp;gt;Quadratic-&amp;gt;Spline으로 갈 수록 모델 복잡도가 올라가는 것을 의미하고, Quadratic이 detail한 부분까지 추정하는 것이 위에서의 언급된 샘플링 시의 변동 분까지 학습한 것이라고 할 수 있습니다. 아래의 두번 째 그림처럼 모델 복잡도를 계속 높이면 train loss는 계속해서 감소하지만, sub set간의 변동 분까지 학습하는 시점부터는 test error가 증가하게 됩니다. 이는 train data의 변동 분까지 학습하여 test data에 대한 추정 값의 variance가 증가한 그림1의 (b)와 같은 상황이라고 할 수 있습니다. 그래서 마지막 그림처럼 모델 복잡도가 올라갈 수록 bias는 감소하나 variance는 증가하는 bias-variance trade off 관계가 되는 것입니다. 마지막 그림에서 MSE는 test data의 total loss로 이해하시면 됩니다. 참고: https://gerardnico.com/wiki/data_mining/bias_trade-off&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://gerardnico.com/wiki/_media/data_mining/bias-variance_trade-off_1.jpg?cache=&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 최적의 모델 복잡도는 위 그림의 세번 째처럼 bias와 variance가 교차하는 부분에서 MSE or total test loss(bias와 variance의 합)가 가장 작은 점의 복잡도를 갖는 것입니다. 즉, 모델의 학습이 train error의 최소가 아닌 test error가 최소가 되도록 해야 한다는 것으로 이해할 수 있습니다. 추가적으로 고려해볼 방법은 처음부터 train set이 갖는 변동을 작게 만드는 것입니다. 이 방법이 바로 n-fold cross validation으로 여러 data set을 만들어 평균적으로 적용시킴으로써 sub set간의 변동을 줄이는 방법이라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;위 그림에서 total loss를 MSE로 표기했습니다. total loss는 bias와 variance loss를 모두 포함하고 있는 것으로 그렸는데 진짜 그럴까요?&lt;/p&gt;

&lt;p&gt;우리가 흔히 사용하는 MSE를 정리하여 다시 구성하면 아래와 같이 variance와 bias의 제곱 텀으로 표현될 수 있다.(sub-set의 변동 분은 제외한 수식입니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-25-Bias_vs_Variance/1.jpg&quot; alt=&quot;MSE&quot; /&gt;&lt;/p&gt;

&lt;p&gt;수식으로 부터 MSE의 variance와 bias의 의미를 다시 살펴보면,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;variance&lt;/strong&gt;는 &lt;u&gt;추정 값의 평균&lt;/u&gt;과 &lt;u&gt;추정 값들&lt;/u&gt;간의 차이에 대한 것이고, &lt;strong&gt;bias&lt;/strong&gt;는 &lt;u&gt;추정값의 평균&lt;/u&gt;과 &lt;u&gt;참 값들&lt;/u&gt;간의 차이에 대한 것입니다.  결국, variance는 추정 값들의 흩어진 정도이고, bias는 참 값과 추정 값의 거리를 의미합니다. 이 수식을 보면 &lt;strong&gt;&lt;u&gt;variance는 loss를 의미하지만, 참 값과는 관계없이 추정 값들의 흩어진 정도만을 의미&lt;/u&gt;&lt;/strong&gt;하고 있음을 유념해야 합니다.&lt;/p&gt;

&lt;p&gt;MSE가 bias와 variance로 구성되어 있고 bias-variance 사이에 trade off 관계인데, 우리가 학습하면 왜 MSE는 작아지고 ‘0’에 가까이 갈까요? 여기서는 trade off 관계가 없는 걸까요?&lt;/p&gt;

&lt;p&gt;위의 MSE의 수식을 보면 bias와 variance가 모두 제곱 텀이므로 둘 다 양수 입니다. 즉 MSE가 고정되면 하나가 커지면 하나는 작아지는 trade off 관계에 있습니다. 그런데 왜 학습을 하면 ‘0’에 가까이 수렴할까요?&lt;/p&gt;

&lt;p&gt;우리는 학습할 때 bias나 variance의 어느 한쪽을 보고 학습하는 것이 아니라 이 둘을 더한 MSE가 작아지도록 학습을 하기 때문에 최적 값을 알아서 찾아 학습을 하게 됩니다.(아마도 두 값이 거의 같을 때 최적 값이 될 것입니다.) 우리가 주로 고민해야하는 것은 train data들의 b-v trade off가 이니라 train 후 test 할 때의 b-v trade off라고 생각하시면 됩니다. 앞서 언급한 샘플링 시 발생하는 sub set의 변동 들에 대해 고려해야 한다는 것입니다.  train 에서 MSE의 최적 점을 찾았는데 train set과 test set의 차이로 test set에서는 이것이 최적점이 아닐 수 있다는 것입니다. (단, train error가 커서 train error에 대한 분석을 할 때는 train의 b-v에 대해서 고민이 필요할 것 같네요.)&lt;/p&gt;

&lt;p&gt;b-v trade off를 한번 더 반복해 보면, 모델 복잡도를 높이면 train data에 대한 bias와 variance는 계속해서 모두 감소합니다(bias+variance이 작아지도록 학습시키므로). 그런데 test data로 평가를 해보면 MSE loss가 어느시점부터 커지는데, 커지는 이유는 test data의 MSE loss 중에서 variance가 다시 커지기 때문이라고 이해하면 좋을 것 같습니다.&lt;/p&gt;

&lt;p&gt;그럼 딥러닝은 엄청나게 복잡도 높은 모델을 만드는데 왜 test data도 잘 맞출 수 있다고 할까요? over-fitting 문제가 없을까요?&lt;/p&gt;

&lt;p&gt;딥러닝도 똑같이 b-v trade off를 피할 수 없습니다. 딥러닝은 그래서 big 데이터가 전제가 되어야 합니다. big 데이터라는 것은 train data가 많다는 것을 의미하고, 이는 거의 전체 데이터(또는 모집단)와 비슷하다고 할수 있으며, 앞서 말한 sampling 변동이 거의 없다는 것을 전제로 하고 있습니다. 그렇기 때문에 모델 복잡도를 도 높일 수 있는 것입니다. 만일 train data가 적은 상태로 복잡한 딥러닝 모델을 적용하면 over-fitting 문제가 발생할 수 있습니다.  전제가 그렇다는 것이고 모든 경우에 big 데이터가 있는 것은 아니여서 딥러닝에서도 이러한 문제를 없애기 위한 다양한 trick(regularization, dropout, domain adaptation 등)이 존재합니다.&lt;/p&gt;

&lt;p&gt;결국, 딥러닝에서의 다양한 trick들은 loss를 줄이기 위한 것이고 , 이 loss는 bias와 variance로 이루어졌다는 것을 기억하면 좋을 것 같습니다. 그리고 그런한 trick들의 효과를 bias-variance 관점에서 생각해보면 다양한 딥러닝 trick들에 대한 더 좋은 이해가 되지 않을까 생각합니다.&lt;/p&gt;

&lt;p&gt;이해 안되는 부분이나 틀린 내용있으면 comment 부탁드립니다.&lt;/p&gt;

&lt;p&gt;다음에 시간이 되면 ensemble 기법인 bagging과 boosting을 사용했을 때 bias, variance가 어떻게 달라지는지 해석해볼 예정입니다.&lt;/p&gt;</content><author><name>홍규석</name></author><category term="개념 정리" /><summary type="html">이 글에서 bias와 variance에 대해 살펴보려고 합니다. bias와 variance는 이미 많은 글이나 블로그에서 개념적으로 잘 설명되어 있습니다. 그럼에도 불구하고 다시 정리해보는 이유는 개념적으로 어느정도 이해는 되는데 좀 더 자세하게 보려고 하면, 블로그들의 예제들 간의 연결이 막혀서 헷갈리는 부분이 있어 이 글을 통해 확실히 이해하기 위해서 입니다.</summary></entry><entry><title type="html">Learning Deconvolution Network for Semantic Segmentation</title><link href="/Learning_Deconvolution_Network_for_Semantic_Segmentation" rel="alternate" type="text/html" title="Learning Deconvolution Network for Semantic Segmentation" /><published>2018-01-03T09:00:00+09:00</published><updated>2018-01-03T09:00:00+09:00</updated><id>/Learning_Deconvolution_Network_for_Semantic_Segmentation</id><content type="html" xml:base="/Learning_Deconvolution_Network_for_Semantic_Segmentation">&lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1505.04366&quot;&gt;Noh, H., Hong, S., and Han, B. Learning Deconvolution Network for Semantic Segmentation. ICCV, 2015.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이번 논문은 &lt;a href=&quot;https://modulabs-biomedical.github.io/FCN&quot;&gt;앞서 다뤘던 Fully Convolutional Networks&lt;/a&gt;와 같은 년도(2015)에 다른 학회(FCN은 CVPR, 본 논문은 ICCV)에 발표된 논문입니다. FCN이나 이후에 다룰 UNet보다는 다소 인기가 적었지만, FCN이 가진 한계를 잘 짚어주셨다는 점에서 공부에 도움이 되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;fcn의-문제점&quot;&gt;FCN의 문제점&lt;/h2&gt;

&lt;h3 id=&quot;크기에-약하다&quot;&gt;크기에 약하다&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 예시들처럼 FCN의 추론 결과를 보면, 대상 물체가 너무 큰 경우(a)에는 파편화되고, 너무 작은 경우(b)에는 배경으로 무시되는 경향이 있습니다. FCN에서는 receptive field(상위 레이어의 한 지점에서 참조하는 하위 레이어의 영역)의 크기가 고정되어, 단일 배율(scale)만을 학습하는 것이 이 문제의 원인이라고 본 논문은 지적합니다. 여러 레이어의 결과를 조합하는 skip 구조가 이러한 현상을 완화시켜주기는 하지만, 근본적인 해법은 아니라는 주장입니다.&lt;/p&gt;

&lt;h3 id=&quot;디테일에-약하다&quot;&gt;디테일에 약하다&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig5.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FCN이 비록 기존 기법들에 비해 큰 발전을 이루었지만, 세부적인 영역을 찾아내는 데에서는 아직 개선의 여지가 있다고 이 논문은 보고 있습니다. FCN에서는 deconvolution에 들어가는 입력부터 이미 세부 묘사가 떨어지고, deconvolution 과정 자체도 충분히 깊지 않고 너무 단순하다고 말합니다.&lt;/p&gt;

&lt;h2 id=&quot;논문의-해법&quot;&gt;논문의 해법&lt;/h2&gt;

&lt;h3 id=&quot;네트워크-구조&quot;&gt;네트워크 구조&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;부족하면 더 넣으면 됩니다. FCN에서는 CNN의 결과를 입력 이미지의 원래 차원으로 확대(upsampling)하는 데에 deconvolution을 사용했지만, 이 논문에서는 deconvolution 시 차원을 유지하는 방법으로, CNN(논문에서 사용한 건 VGG-16)의 convolution만큼 레이어 숫자를 늘렸습니다. 결과적으로 거울에 비춘 모양이 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CNN으로 인해 원래 이미지보다 축소된 차원 크기는 unpooling으로 복원합니다. 여기서 unpooling이란 CNN의 max pooling 시의 위치 정보를 기억했다가, 원래 위치로 그대로 복원해주는 작업입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 효과는 위의 그림과 같습니다. (b)에서 (c)로 갈 때의 unpooling에 의해, 해상도가 커지는 대신 신호가 흩어져서 희소(sparse)해집니다. 이것을 (c)에서 (d)로 deconvolution을 거치면, 디테일을 살려내면서 신호가 고르게 밀집(dense)됩니다. 이러한 과정이 반복되자 노이즈도 점차 자연스럽게 사라지는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;학습-및-추론-방식&quot;&gt;학습 및 추론 방식&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/edge-box.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;단일 데이터셋에서 다양한 크기의 사례들을 학습하기 위해, 논문에서는 &lt;a href=&quot;&quot;&gt;edge-box&lt;/a&gt;라는 object proposal 알고리즘을 사용하여 무언가 있을만한 영역을 다양한 크기의 상자로 골라냅니다. 학습 시에는 우선 실제 정답이 가운데에 들어가도록 잘라낸(crop) 이미지들로 1차 학습을, 그 다음 edge-box의 결과물 중 실제 정답과 잘 겹치는 것들을 활용하여 조금 더 심도있는 2차 학습을 진행합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig6.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 학습에 사용된 edge-box는 추론 시에도 사용되는데, 추론 시 사용하는 object proposal의 수(상자 수)를 증가시킬 수록 성능은 좋아진다고 합니다. 물론 그만큼 계산량과 시간은 늘어납니다.&lt;/p&gt;

&lt;h2 id=&quot;결과&quot;&gt;결과&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig7.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 세심하게 설계되고 학습된 결과는 FCN이 실수하는 물체들도 보다 세밀하게 잘 찾아내는 모습을 보입니다. 다만 FCN이 잘 맞추는 곳에서 실수를 할 때도 있는데, 결국 둘을 앙상블하여 conditional random field로 후처리하면 두 가지 모델을 모두 뛰어넘게 되어, FCN과 상호 보완적인 관계에 있다고 논문은 맺습니다.&lt;/p&gt;

&lt;h2 id=&quot;추가-참고-문헌&quot;&gt;추가 참고 문헌&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.matthewzeiler.com/wp-content/uploads/2017/07/cvpr2010.pdf&quot;&gt;Zeiler, M. D. et al. Deconvolutional Networks. CVPR, 2010.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/edge-boxes-locating-object-proposals-from-edges/&quot;&gt;Zitnick, L. and Dollar, P. Edge Boxes: Locating Object Proposals from Edges. ECCV, 2014.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>오상준</name></author><category term="논문 리뷰" /><summary type="html">Noh, H., Hong, S., and Han, B. Learning Deconvolution Network for Semantic Segmentation. ICCV, 2015.</summary></entry><entry><title type="html">Fully Convolutional Networks for Semantic Segmentation</title><link href="/FCN" rel="alternate" type="text/html" title="Fully Convolutional Networks for Semantic Segmentation" /><published>2017-12-21T19:00:00+09:00</published><updated>2017-12-21T19:00:00+09:00</updated><id>/FCN</id><content type="html" xml:base="/FCN">&lt;p&gt;논문 링크 : &lt;a href=&quot;https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&quot;&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Semantic Segmentation는 영상을 pixel단위로 어떤 object인지 classification 하는 것이라고 볼 수 있습니다. (언제나 강력추천하는) &lt;a href=&quot;http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf&quot;&gt;cs231n 강의 자료&lt;/a&gt;를 보시면 쉽게 잘 나와 있죠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig1.jpg&quot; alt=&quot;&quot; /&gt;
Figure 1. Computer Vision Tasks&lt;/p&gt;

&lt;p&gt;예전에는 이렇게 pixel 단위로 classification을 하기 위해,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;그림 2와 같이 일정 영역을 포함하는 window를 만들고,&lt;/li&gt;
  &lt;li&gt;window 내 영상의 object를 classifiy해서&lt;/li&gt;
  &lt;li&gt;window 중앙의 pixel의 class 값이라고 간주하는&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;방식을 주로 사용했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig2.jpg&quot; alt=&quot;&quot; /&gt;
Figure 2.&lt;/p&gt;
&lt;strike&gt; 그림이 필요해서 커피숖에서 작업중에 급조를...&lt;/strike&gt;

&lt;p&gt;당연히 
&lt;strong&gt;계산량의 문제&lt;/strong&gt; 와 global information을 사용하지 못하고 window라는 제한된 영역의 &lt;strong&gt;local information만 사용한다&lt;/strong&gt; 는 문제가 있을 겁니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 
local feature와 global feature를 모두 사용하고
계산량 잡아먹는 주범인 fully connected layer를 없앤 CNN architecture를 제안합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h2&gt;
&lt;p&gt;Fully Convolutional Networks의 구조는 다음 그림 3과 같으며, 크게 4가지 부분으로 구성이 된다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig3.jpg&quot; alt=&quot;&quot; /&gt;
Figure 3. Architecture of Fully Convolutional Networks&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;네트워크 구조&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;(1) Feature Extraction&lt;/strong&gt; : 
 일반적인 CNN의 구조에서 많이 보이는 conv layer들로 구성되어 있습니다. 
 &lt;strong&gt;(2) Feature-level Classification&lt;/strong&gt; : 
 추출된 Feature map의 pixel 하나하나마다 classification을 수행합니다. 이 때 classification된 결과는 매우 coarse합니다. (그림 3에서 초록색 박스에 tabby cat class에 대한 Classification 결과 참고) 
 &lt;strong&gt;(3) Upsampling&lt;/strong&gt; :
 coarse 한 결과를 backward strided convolution 을 통해 upsampling하여 원래의 image size로 키워줍니다. 
&lt;strong&gt;(4) Segmentation&lt;/strong&gt; : 
각 class의 upsampling된 결과를 사용하여 하나의 Segmentation  결과 이미지를 만들어 줍니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그러면, 각 네트워크의 내부들을 한번 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;####&lt;em&gt;Feature Extraction&lt;/em&gt;
&lt;a href=&quot;https://arxiv.org/abs/1409.1556&quot;&gt;VGG-19 network&lt;/a&gt;를 feature extractor로 사용한다고 가정을 해봅시다. 이 경우 conv1 ~ conv5 layer ( or pool5 layer) 까지 통과하면서 feature를 추출합니다. 낮은 layer의 경우 작은 receptive field를 지니므로 작은 크기의 feature가, 높은 layer의 경우 높은 receptive field를 지니므로 큰 크기의 feature가 추출되게 되죠.
이렇게 추출된 최종 feature map (conv5 or pool5 layer)을 이용하여 다음 단계에서 coarse한 segmentation map 을 만들어냅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig4.jpg&quot; alt=&quot;&quot; /&gt;
Figure 4. Feature Extraction on VGG-19 Networks&lt;/p&gt;

&lt;h4 id=&quot;feature-level-classification&quot;&gt;&lt;em&gt;Feature-level Classification&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;원래 VGG network는 이렇게 추출된 feature의 뒤에 4096, 4096, 1000으로 이어지는 fully connected layer를 연결하여 classification을 합니다만(그림 4), 본 논문에서는 이런 fully connected layer를 없애버립니다. 그리고, 1x1 conv layer를 추가합니다. 그림 3에 보시면 4096, 4096, 21 이라고 표현된 1x1 conv layer를 보실 수 있는데요. 
(1000이 21로 바뀐 이유는 이 논문에서는 PASCAL VOC dataset으로 실험을 하는데, 그 데이터의 클래스가 20개 + background 이기 때문입니다.)
이 1x1 conv 의 결과물이 결국 각 class의 feature map 상에서의 classifiation (즉, segmentation) 이 됩니다. 그림 3에 보면 conv8(마지막 1x1 conv) layer의 depth channel 중에서 tabby cat에 해당하는 class의 feature map  상에서의 classification (즉, segmentation) 결과 heatmap 을 볼 수 있습니다. 마지막 1x1 conv layer에서 depth channel은 각 class를 의미하므로, 어떤 class의 segmentation heatmap도 추정할 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;upsampling&quot;&gt;&lt;em&gt;Upsampling&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;그런데 feature map level에서 segmentation 한 결과는 너무 coarse한 결과입니다. (그림 3의 tabby cat heatmap 보면 깍두기처럼…) 따라서, 이 coarse한 heatmap을 dense하게 (원래의 image size로) 만들어주어야 합니다. 본 논문에서는 upsampling(backwards strided convolution)을 사용합니다. 
그러면 각 class별로 dense한 segmentation 결과를 얻을 수 있습니다. 즉, 원래 image의 폭을 W, 높이를 H, 라고 한다면, WxHx21 의 dense heatmap결과를 얻을 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;segmentation&quot;&gt;&lt;em&gt;Segmentation&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;그러나 우리는 결국 각 class 별 결과를 추정하고자 하는 것이 아니죠. 하나의 이미지에서 모든 class의 segmentation된 결과를 얻어야 합니다. 그래서 윗 단계에서 얻어진 upsampling된 각 class별 heatmap을 softmax를 이용하여 가장 높은 확률을 가지는 class만 모아서 한장의 segmentation 이미지로 만듭니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;skip-combining&quot;&gt;Skip Combining&lt;/h2&gt;
&lt;p&gt;그런데 3단계의 &lt;strong&gt;&lt;em&gt;Upsampling&lt;/em&gt;&lt;/strong&gt; 과정에서 coarse한 결과를 dense하게 만들어줄 때 너무 많이 뻥튀기를 하기 때문에 detail아 다 뭉개진 segmenation 결과를 얻을 수 밖에 없습니다. (그림 5 참고)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig5.jpg&quot; alt=&quot;&quot; /&gt;
Figure 5. Segmentation Result from the Last Conv Layer&lt;/p&gt;

&lt;p&gt;다음 그림 6과 같은 CNN 구조의 Fully Convolutional Networks가 있다고 가정해봅시다. 최종 결과물인 FCN-32s는 32배로 upsampling을 하기 때문에 detail이 많이 사라진 segmentation 결과를 보여줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig6.jpg&quot; alt=&quot;&quot; /&gt;
Figure 6. (Conventional) Fully Convolutional Networks : FCN-32s&lt;/p&gt;

&lt;p&gt;본 논문에서는 이 문제를 해결하기 위해 그 이전 layer의 feature map을 이용하는 skip combining 기법을 사용합니다(그림 7 참고).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig7.jpg&quot; alt=&quot;&quot; /&gt;
Figure 7. Fully Convolutional Networks with Skip Combining : FCN-16s&lt;/p&gt;

&lt;p&gt;그림 6에서는 마지막 conv layer인 conv 7에서 32배 upsampling하여 segmentation 결과를 만들었습니다. 하지만, 그림 7에서는 마지막 conv layer 결과를 2배 upsampling 하고 마지막 pooling layer (pool5)이전 단계 즉, pool 4 layer의 결과와 합쳐줍니다. 그리고 난 후, 그 합쳐진 결과를 16배 upsampling 하여 FCN-16s 라는 segmentation 결과 이미지를 만들어 냅니다.&lt;/p&gt;

&lt;p&gt;여기서 합친다는 말은 그냥 &lt;strong&gt;더해준다&lt;/strong&gt;는 의미입니다. 
&lt;a href=&quot;https://github.com/shekkizh/FCN.tensorflow/blob/master/FCN.py&quot;&gt;다음 코드&lt;/a&gt;를 참고하세요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig7.code.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 하나 더 이전의 pooling layer와도 합칠 수 있지 않을까요? 당연히 있습니다.
그림 8은 pool 3 layer + 2배 upsampling된 pool 4 layer + 4배 upsampling 된 conv7 layer 값을 다시 8배 upsampling 하여 FCN-8s라는 보다 detail한 segmentation 이미지를 만들어내는 구조입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig8.jpg&quot; alt=&quot;&quot; /&gt;
Figure 8. Fully Convolutional Networks with Skip Combining : FCN-8s&lt;/p&gt;

&lt;p&gt;자 그러면 각 skip combining 한 후의 최종 segmentation 결과를 살펴볼까요? 확실히 FCN-32s에 비해 FCN-16s가, FCN-16s에 비해 FCN-8s가 detail한 segmentation 결과를 보여줌을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2017-12-21-FCN/fig9.jpg&quot; alt=&quot;&quot; /&gt;
Figure 9. Segmentation Results&lt;/p&gt;</content><author><name>김승일</name></author><category term="논문 리뷰" /><summary type="html">논문 링크 : Fully Convolutional Networks for Semantic Segmentation</summary></entry></feed>
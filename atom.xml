<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> - Articles</title>
    <description>모두의연구소 바이오메디컬랩 기술 블로그입니다.</description>
    <link>
    http://localhost:4000</link>
    
      
      <item>
        <title>TensorPack</title>
        
          <description>&lt;h2 id=&quot;tensorpack-구조-이해하기&quot;&gt;Tensorpack 구조 이해하기&lt;/h2&gt;

</description>
        
        <pubDate>Fri, 17 Aug 2018 09:00:00 +0900</pubDate>
        <link>
        http://localhost:4000/TensorPack</link>
        <guid isPermaLink="true">http://localhost:4000/TensorPack</guid>
      </item>
      
    
      
      <item>
        <title>FusionNet</title>
        
          <description>&lt;h1 id=&quot;fusionnet&quot;&gt;&lt;strong&gt;FusionNet&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;FusionNet은 U-Net처럼 Semantic Segmentation에 활용 할 수 있는 모델입니다.&lt;/p&gt;

&lt;p&gt;이름이 FusionNet인 이유는 아마도 Encoder에 있는 Layer를 가져와 Decoder에 결합(Fusion)하는 방법이 이 모델에 가장 특징적인 부분이기 때문인 것 같습니다.&lt;/p&gt;

&lt;p&gt;U-Net과 유사한 부분이 많기 때문에 블로그에 U-Net글을 읽으시면 많은 도움이 될 것 같습니다. 
&lt;a href=&quot;https://modulabs-biomedical.github.io/U_Net&quot;&gt;U-Net (by 강은숙)&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;fusionnet-이란&quot;&gt;FusionNet 이란?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;FusionNet&lt;/strong&gt;은 Connectomics(뇌신경 연결지도를 작성하고 분석하는 신경과학의 일종) 데이터에서 신경구조를 Segmentation 할 목적으로 만든 딥러닝 모델입니다.&lt;/p&gt;

&lt;p&gt;논문발표일(2016.12.26) 기준에서 최신의 기술인 Semantic Segmentation과 Residual Neural Networks를 사용 한 모델이라고 하네요.&lt;/p&gt;

&lt;p&gt;이 논문에서는 FusionNet을 Electron Microscopy (EM) 이미지에서 Cell Membrane과 Cell Body의 Segmentation에 적용하였습니다.&lt;/p&gt;

&lt;h3 id=&quot;모델의-특징&quot;&gt;모델의 특징&lt;/h3&gt;

&lt;p&gt;이전에 connectomics에서 automatic image segmentation을 할 때 보통 &lt;strong&gt;CNN(Convolutional Neural Network)&lt;/strong&gt;에 기반 한 &lt;strong&gt;patch-based pixel-wise classification&lt;/strong&gt;을 사용했는데 EM 데이터가 용량이 상당히 크기 때문에 처리하기 위한 비용이 많이 들어간다는 문제가 있었습니다.&lt;/p&gt;

&lt;p&gt;( 여기서 비용이란 딥러닝을 하기 위한 하드웨어 구입, 전기 사용, 시간 등을 말합니다. )&lt;/p&gt;

&lt;p&gt;Encoding, Decoding을 사용한 &lt;strong&gt;FCN(fully Convolution Neural Network)&lt;/strong&gt;계열의 모델의 경우 연산량을 줄일 수 있어 비용을 절감 할 수 있는데 이것에 대표적인 모델이 &lt;strong&gt;U-Net&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p&gt;하지만 &lt;strong&gt;U-Net&lt;/strong&gt;의 방법이 데이터로부터 다중의 맥락 정보를 학습할 수 있지만 &lt;strong&gt;Gradient Vanising&lt;/strong&gt; 문제가 발생하기 때문에 깊은 네트워크를 만드는 것은 제한적이었습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FusionNet&lt;/strong&gt;은 &lt;strong&gt;residual layers&lt;/strong&gt;와 &lt;strong&gt;summation-based skip connection&lt;/strong&gt;을 사용하여 &lt;strong&gt;U-Net&lt;/strong&gt;의 단점을 보완한 모델입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;residual layer&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;이전에 계산된 값을 뒷부분에도 연결시켜서 모델이 깊어져도 값을 잊어버리지 않게 해주는 역할을 함 (vanishing gradient 문제 해결)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;summation-based skip connections&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;층을 건너띄워서 연결, 건너띄워 넘어온 층과 이전의 층을 더해서(합계산) 뒤에 층으로 넘김&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;pytorch-code-예시-summation-based-skip-connections&quot;&gt;Pytorch Code 예시 (summation-based skip connections)&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-05-04-Fusion_Net/fusion-net_1.png&quot; alt=&quot;fusion-net_1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;u-net과-비교하여-가장-특징적인-차이&quot;&gt;U-Net과 비교하여 가장 특징적인 차이&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;U-net&lt;/strong&gt;에서는 긴 skip connection을 통해 feature map을 &lt;strong&gt;이어 붙임 (Concatenating)&lt;/strong&gt; 
&lt;strong&gt;FusionNet&lt;/strong&gt;에서는 긴 skip connection을 통해 feature map의 &lt;strong&gt;값을 더함&lt;/strong&gt; + 짧은 skip connection도 encoding, decoding의 각 step에서 사용&lt;/p&gt;

&lt;p&gt;이 방법이 실제로 좋은 퍼포먼스를 보여서 &lt;strong&gt;ISBI 2012 EM segmentation challenge&lt;/strong&gt;에서 &lt;strong&gt;1등&lt;/strong&gt; 했다고 합니다.
(물론 FusionNet이 100% 기여했다고 까지는 볼 수 없을 것 같습니다.)&lt;/p&gt;

&lt;p&gt;OverFitting 해결을 위한 &lt;strong&gt;Data enrichment&lt;/strong&gt; (이미지 데이터에 다양한 변형을 가하거나 노이즈를 추가하는 등의 방법을 사용)를 사용 한 것도 좋은 퍼포먼스를 내는데 도움이 되었을 것 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-05-04-Fusion_Net/fusion-net_2.png&quot; alt=&quot;fusion-net_2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;정말 U-Net보다 결과가 좋네요!!!&lt;/p&gt;

&lt;h3 id=&quot;모델-아키텍처&quot;&gt;모델 아키텍처&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-05-04-Fusion_Net/fusion-net_3.png&quot; alt=&quot;fusion-net_3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Encoding Path&lt;/strong&gt;	-&amp;gt;	640X640부터 40X40 까지 the features of interest를 검출&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Decoding Path&lt;/strong&gt;	-&amp;gt;	40X40부터 640X640까지 synthesis를 예측&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;녹색 블록&lt;/strong&gt;	-&amp;gt;	regular convolutional layer, ReLu 활성함수, batch normalization으로 구성&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;보라색 블록&lt;/strong&gt;	-&amp;gt;	3개의 convolutional block으로 구성되는데 마지막 블록3에는 블록1이 블록2와 skip해서 더해진 residual layer가 연결 됨&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;파란색 블록&lt;/strong&gt;	-&amp;gt;	maxpooling layer로써 encoding path에서 feature 압축을 위한 downsampling을 하기 위해 사용&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;빨간색 블록&lt;/strong&gt;	-&amp;gt;	deconvolutional layer로 decoding path에서 interpolation을 사용한 upsampling을 하기 위해 사용&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/posts/2018-05-04-Fusion_Net/fusion-net_4.png&quot; alt=&quot;fusion-net_4&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;참고자료&quot;&gt;참고자료&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/ftp/arxiv/papers/1612/1612.05360.pdf&quot;&gt;FusionNet : A deep fully residual convolutional neural network for image segmentation in connectomics&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이상으로 FusionNet에 대한 글을 마칩니다. 수정해야 할 내용이 있다면 꼭 말씀 부탁드립니다.
부족한 글이지만 끝까지 읽어주셔서 감사합니다!!!&lt;/p&gt;
</description>
        
        <pubDate>Fri, 04 May 2018 09:00:00 +0900</pubDate>
        <link>
        http://localhost:4000/FusionNet</link>
        <guid isPermaLink="true">http://localhost:4000/FusionNet</guid>
      </item>
      
    
      
      <item>
        <title>U-Net</title>
        
          <description>&lt;p&gt;논문 링크 : &lt;a href=&quot;https://arxiv.org/pdf/1505.04597.pdf&quot;&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/a&gt;&lt;/p&gt;

</description>
        
        <pubDate>Mon, 02 Apr 2018 09:00:00 +0900</pubDate>
        <link>
        http://localhost:4000/U_Net</link>
        <guid isPermaLink="true">http://localhost:4000/U_Net</guid>
      </item>
      
    
      
      <item>
        <title>Bias vs. Variance 개념 정리</title>
        
          <description>&lt;p&gt;이 글에서 bias와 variance에 대해 살펴보려고 합니다. bias와 variance는 이미 많은 글이나 블로그에서 개념적으로 잘 설명되어 있습니다. 그럼에도 불구하고 다시 정리해보는 이유는 개념적으로 어느정도 이해는 되는데 좀 더 자세하게 보려고 하면, 블로그들의 예제들 간의 연결이 막혀서 헷갈리는 부분이 있어 이 글을 통해 확실히 이해하기 위해서 입니다.&lt;/p&gt;

</description>
        
        <pubDate>Thu, 25 Jan 2018 09:00:00 +0900</pubDate>
        <link>
        http://localhost:4000/Bias_vs_Variance</link>
        <guid isPermaLink="true">http://localhost:4000/Bias_vs_Variance</guid>
      </item>
      
    
      
      <item>
        <title>Learning Deconvolution Network for Semantic Segmentation</title>
        
          <description>&lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1505.04366&quot;&gt;Noh, H., Hong, S., and Han, B. Learning Deconvolution Network for Semantic Segmentation. ICCV, 2015.&lt;/a&gt;&lt;/p&gt;

</description>
        
        <pubDate>Wed, 03 Jan 2018 09:00:00 +0900</pubDate>
        <link>
        http://localhost:4000/Learning_Deconvolution_Network_for_Semantic_Segmentation</link>
        <guid isPermaLink="true">http://localhost:4000/Learning_Deconvolution_Network_for_Semantic_Segmentation</guid>
      </item>
      
    
      
      <item>
        <title>Fully Convolutional Networks for Semantic Segmentation</title>
        
          <description>&lt;p&gt;논문 링크 : &lt;a href=&quot;https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&quot;&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;

</description>
        
        <pubDate>Thu, 21 Dec 2017 19:00:00 +0900</pubDate>
        <link>
        http://localhost:4000/FCN</link>
        <guid isPermaLink="true">http://localhost:4000/FCN</guid>
      </item>
      
    
  </channel>
</rss>
